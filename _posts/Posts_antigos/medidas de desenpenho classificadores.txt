2017-10-05

Imagine que você possui um classificador atuando em uma determinada base de dados e necessita avaliar o seu desempenho. Como você faria? A primeira resposta, e a mais óbvia, seria: ora, basta verificar quantas amostras ele consegue acertar! De fato essa é uma métrica. Todavia, pode ser que somente ela não seja suficiente para identificar os pontos fortes e fracos do seu classificador. Por isso, neste post vamos discutir outras métricas para avaliar o desempenho de classificadores. Caso não esteja familiarizado com o tema, sugiro o nosso post anterior que define o <a href="http://www.computacaointeligente.com.br/artigos/classificacao-de-dados/" target="_blank" rel="noopener">problema de classificação de dados.</a>

Para tornar nossa discusão mais didática e compreensível, vamos utilizar como exemplo o <a href="http://archive.ics.uci.edu/ml/datasets/Breast+Cancer" target="_blank" rel="noopener"><em>Breast Cancer Data Set</em></a>, um benchmark clássico em classificação de dados. Esta base de dados possui 9 atributos descrevendo 286 mulheres que sobreviveram as câncer de mama e se após 5 anos a doença retornou. Logo, esse é um problema de classificação binária, na qual o classificador deve responder se o câncer voltou ou não. Das 286 mulheres, em 85 o câncer retornou e em 201 não. Portanto, essa base será nosso ponto de partida.
<h4>Acurácia de classificação</h4>
Como já mencionado, a primeira métrica a se utilizar é a acurácia de classificação. Ela é bem simples de ser calculada, basta verificar a porcentagem de acerto do classificador. Por exemplo, se das 286 amostras da base de cancer, o classificador acertou 230, teremos uma acurácia final de (230/286) x 100 = 80,42%. Agora uma pergunta: baseado nessa acurácia, este classificador é bom ou ruim? Acredito que a melhor resposta até então seria <em>não sei</em>. Temos que analisar melhor esse resultado e entender o que é importante para o problema em questão. Apenas saber o valor da acurácia não é suficiente para declarar a qualidade desse classificador.

Para justificar essa afirmativa, vamos pensar um pouco em relação a base de dados em questão. Queremos saber se o câncer retornou em uma mulher ou não, certo? Logo, se for para o classificador errar, a situação "<em>menos pior" </em>é dizer que o câncer de uma mulher retornou mas na verdade ele não retornou. Neste caso, temos um erro de classificação, mas não é o mais grave porque neste caso, em uma situação imaginária, a mulher retornaria para o hospital, realizaria alguns exames e identificaria que o câncer na verdade não retornou e assim poderia ser liberada. Sendo assim, o pior cenário para este classificador e dizer que a mulher não retornou o câncer quando na verdade o câncer retornou. Baseado na mesma situação imaginária, a mulher poderia estar em casa imaginando não estar com câncer quando na verdade o mesmo retornou. Portanto, precisamos de uma métrica para identificar esse tipo de situação.
<h4>Matriz de confusão</h4>
A matriz de confusão nada mais é do que uma tabela que permite visualizar o desempenho do classificador. Cada linha da matriz representa a classe das amostras que o classificador retornou e cada coluna representa a classe real. Para facilitar a compreensão, vamos construir uma matriz de confusão fictícia para o problema da base de câncer:

[supsystic-tables id=1]

A pergunta a ser repondida pelo classificador é: o câncer da amostra retornou? Logo, o mesmo classifica em Sim ou Não. Para não criar uma confusão da matriz de confusão, olhe para a diagonal principal. Das 85 amostras nas quais o câncer retornou, nosso classificador fictício rotulou corretamente 60. Similarmente, das 201 nas quais o câncer não retornou, ele classificou corretamente 190. Essa informação encontra-se na diagonal principal. Perceba que a soma da linha e coluna entitulada 'Total' obrigatoriamente deve ser igual ao número de amostras, no caso, 286. Por fim, é possível perceber que dos 36 erros do classificador, 25 atingem o caso que discutimos anteriormente (dizer que o câncer não retornou, quando na verdade ele retornou).

Existe uma nomenclatura específica para lidar com os acertos e erros de classificadores e vamos utilizar a matriz de confusão para expor melhor essa ideia. As 60 amostras preditas corretamente serão denominadas como <strong>verdadeiros positivos (VP)</strong>, pois o resultado do classificador respondeu a pergunta em questão com <em>Sim </em>(positivo) e ela é correta (verdadeira). As 25 classificadas com <em>Sim</em>, mas na verdade é <em>Não</em>, é são denominadas <strong>falsos positivos (FP)</strong>. De maneira similar, na segunda linha da tabela encontramos 11 amostras classficadas como <em>Não</em>, mas que na verdade é <em>Sim</em>. Essas são os <strong>falsos negativos (FN)</strong>. Por fim, as 190 amostras <em>Não </em>classificadas corretamente são os <strong>verdadeiros negativo (VN)</strong>, pois neste caso a resposta do classificador é correta para classe <em>Não</em> em questão (se você se embolou, leia novamente com mais calma que conseguirá entender).

Entendeu essa nova nomenclatura? Pois bem, a partir dela virão as novas métricas para medir o desempenho dos classificadores.
<h4><em>Precision</em></h4>
A métrica <em>precision</em> (não vou traduzir para precisão pois realmente eu não sei se é traduzido para este novo. Provavelmente sim, mas eu nunca vi) tem como objetivo identificar quantas amostras foram classificadas positivamente. Em outras palavras, é uma medida do quão exato é a classificação para as amostras positivas, que exatamente a pergunta que desejamos responder. Por conta disso, a métrica também é conhecida como <strong>valor de predição positiva</strong>. Ela é definida como o número de <strong>verdadeiros positivos </strong>divido pela soma de <strong>verdadeiros positivos </strong>mais <strong>falsos positivos</strong>:
<p style="text-align: center;"> $latex P = \frac{VP}{VP + FP}$</p>
De maneira clara, podemos perceber que um baixo valor de P, pode indicar um grande número de falsos positivos na classificação geral. No nosso exemplo da tabela acima, temos  $latex P = \frac{60}{(60+25)} = 0.7059$. Como discutimos acima, essa métrica é a mais importante para este problema, pois queremos ao máximo evitar os <strong>falsos positivos</strong>, ou seja, dizer que a o câncer não voltou, quando na verdade ele voltou.
<h4><em>Recall</em></h4>
O <em>recall</em>, também conhecido como taxa de sensibilidade ou taxa de verdadeiro positivo, tem a mesma ideia da <em>precision</em><em>, </em>porém para as amostras <strong>falsas negativas</strong>. Ela é definida como o número de <strong>verdadeiros positivos </strong>divido pela soma de <strong>verdadeiro positivos</strong> mais <strong>falso negativos</strong>:
<p style="text-align: center;">$latex R = \frac{VP}{VP + FN}$</p>
O <em>recall</em> pode ser compreendido como uma medida do quão completo é o nosso classificador. Obviamente, um baixo valor de R indica um alto valor de <strong>falsos positivos</strong>. Para o nosso problema, temos  $latex R = \frac{60}{(60+11)} = 0.8451$.
<h4><em>F1 score</em></h4>
O <em>F1 score</em> é definido como duas vezes a média harmônica entre <em>R e P</em>, ou seja, é um 'meio termo' entre as duas métricas anteriores:
<p style="text-align: center;">$latex F1 = 2 \times \frac{P \times R}{P + R}$</p>
Neste caso, essa métrica busca um modelo que faça um balanço entre <em>recall </em>e <em>precision</em>. Para o nosso exemplo, temos $latex F1 = 2 \times \frac{0.7059 \times 0.8451}{0.7059 + 0.8451} = 0.7693$
<h4>Acurácia</h4>
Perceba que podemos calcular a acurácia total do nosso classificador da seguinte forma:
<p style="text-align: center;">$latex A = \frac{VP + VN}{VP + VN + FP + FN}$</p>
Essa é a mesma métrica apresentado no início deste post, todavia, agora utilizando a nomenclatura introduzida com a matriz de confusão.

Perceba que para cada problema, nós poderemos priorizar uma métrica em relação a outra. Para o problema do câncer, devemos priorizar a <em>precision</em> (é já explicamos a razão). Todavia, isso não significa negligenciar as demais.
<h3>Problemas com multiplos labels</h3>
Neste momento você pode estar se pensando: ok, entendi toda essa teoria para o problema exemplo, porém e se o meu problema tiver mais de dois labels, ou seja, labels A, B, C... (não apenas SIM e NÃO), como devo proceder? Bom, não se preocupe o cálculo das métricas continua bem fácil. Vamos considerar um exemplo os três labels A, B e C. Primeiro, vamos calcular a matriz de confusão, como mostrado a seguir:

[supsystic-tables id=2]

A ideia é exatamente a mesma da anterior, porém dessa vez temos 3 classes (mas nada impede de termos mais). Perceba que para este exemplo cada label possui 100 amostras (soma das colunas) e cada label predito (o resultado da classificação) possui 60, 120 e 100 (soma das linhas). Além disso, a diagonal principal da matriz é composta pelos <strong>verdadeiros positivos</strong><strong> (VP)</strong>, ou seja, os valores corretos da predição.

A partir dessas informações podemos calcular todas as métricas já apresentadas. Neste caso, temos que calcular a métrica para cada label. Tomando como exemplo o label A, vamos calcular as métricas da seguinte forma: sabemos que  $latex P = \frac{VP}{VP + FP}$, logo VP = 30 e VP + FP é igual a soma da linha de A, ou seja, 30+20+10=60. Portanto, PA = 0.5. Isso significa, quem 50% do tempo o modelo prever corretamente o valor do label A.

De maneira similar, vamos calcular o <em>recall </em>para o label A. Sabemos que $latex R = \frac{VP}{VP + FN}$, logo VP = 30 e VP + FN é igual a soma da coluna  de A, ou seja, 30+50+20=100. Portanto, RA = 0.3. Isso significa que em 70% dos casos que era pro modelo prever B ou C, o resultado é A, ou seja, só em 30% das vezes o modelo acerta nessa situação (o que é bem ruim).

Bom, para calcular o <em>F1 score</em>, basta duplicar a média harmônica entre P e R. De maneira análoga, o mesmo procedimento pode ser realizado para os labels B e C. Daí seria computado PB, RB, PC e RC. Quando computado as métricas para todos os labels, você pode calcular o <em>recall e precision</em> médios simplesmente utilizando a média aritimética dos valores.

&nbsp;

Agora, daqui a diante você possui condições de medir melhor o desempenho dos seus modelos de classificação.

&nbsp;
<h4>Referências</h4>
[1] Zhang, Min-Ling, and Zhi-Hua Zhou. "A review on multi-label learning algorithms." <i>IEEE transactions on knowledge and data engineering</i> 26.8 (2014): 1819-1837.

[2] Classification Accuracy is Not Enough: More Performance Measures You Can Use - <a href="https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/" target="_blank" rel="noopener">Link</a>

[3] Computing Precision and Recall for Multi-Class Classification Problems - <a href="http://text-analytics101.rxnlp.com/2014/10/computing-precision-and-recall-for.html" target="_blank" rel="noopener">Link</a>