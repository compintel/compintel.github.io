I"¸!<h1 id="introdu√ß√£o">Introdu√ß√£o</h1>

<p>Hoje vamos¬†apresentar¬†a M√°quina de Aprendizado Extremo(do ingl√™s: <em>Extreme Learning Machine</em>, ELM), algoritmo proposto por Huang et al. <a href="#huang">[1,2]</a> e que nada mais √© do que uma maneira diferente de treinar uma <a href="/artigos/redes-neurais-artificiais/">rede neural artificial (RNA)</a> de apenas uma camada oculta.¬†O princ√≠pio de funcionamento da ELM √© o mesmo de uma RNA, todavia a metodologia de treinamento de uma ELM n√£o √© baseada em gradiente descendente. Com isso o algoritmo escapa das principais defici√™ncias do <em>backpropagation</em>: converg√™ncia lenta e converg√™ncia para m√≠nimos locais. Segundo Huand et al. <a href="#huang2006">[2]</a> o treinamento de uma ELM pode ser milhares de vezes mais r√°pido do que o treinamento via <em>backpropagation</em> (e muitas das vezes, de fato √©). Para ilustras a arquitetura de uma ELM, podemos utilizar a mesma figura do post de RNA, por√©m com k = 1, ou seja, temos apenas uma camada oculta.</p>

<figure style="width: 500px; height: 400px" class="align-center">
  <img src="http://0.0.0.0:4000/assets/img/posts/redes-neurais/redeFeed.png" />
  <figcaption style="text-align: center;">
    Figura 1: A ELM nada mais √© do que uma RNA com apenas uma camada oculta, ou seja, k = 1
  </figcaption>
</figure>

<h1 id="modelagem-matem√°tica">Modelagem matem√°tica</h1>

<p>Observando a Figura 1, o vetor \(X\) √© a entrada da rede. Os pesos de conex√£o da camada de entrada s√£o alocados em uma matriz denominada \(W\) e j√° os da camada oculta em uma matriz denominada \(\beta\). Para facilitar e agilizar os c√°lculos, os <em>bias</em> dos neur√¥nios da camada oculta tamb√©m s√£o alocados na √∫ltima linha de \(W\), e os <em>bias</em> da camada de sa√≠da n√£o s√£o utilizados na ELM. A modelagem matricial do ELM √© descrita a seguir:</p>

\[\mathbf{X} = [x_1, \cdots, x_m, 1]
\qquad
\mathbf{W} = \begin{bmatrix}
w_{11}&amp; \cdots &amp; w_{1d} \\
\vdots &amp; \ddots &amp; \vdots \\
w_{m1}&amp; \cdots &amp; w_{md} \\
b_1 &amp; \cdots &amp; b_d
\end{bmatrix}

\tag{1}\]

\[\boldsymbol{\beta} = \begin{bmatrix}
\beta_{11}&amp; \cdots &amp; \beta_{1s} \\
\vdots &amp; \ddots &amp; \vdots \\
\beta_{d1}&amp; \cdots &amp; \beta_{ds}
\end{bmatrix}
\qquad
\mathbf{Y} = [y_1, \cdots , y_s]

\tag{2}\]

<p>Sendo, \(m\) √© o n√∫mero de neur√¥nios de entradas, \(d\) √© o n√∫mero de neur√¥nios na camada oculta e \(s\) √© o n√∫mero de neur√¥nios de sa√≠da da rede.</p>

<h1 id="treinamento-da-elm">Treinamento da ELM</h1>

<p>O treinamento da ELM √© realizado de maneira anal√≠tica, diferentemente da abordagem iterativa do <em>backpropagation</em>. A matriz \(W\) √© gerada de maneira aleat√≥ria e n√£o √© alterada at√© o fim do algoritmo. Essa matriz pode ser gerada de uma distribui√ß√£o uniforme no intervalo [-1,1]. Portanto, o objetivo do treinamento da ELM √© encontrar a matriz de pesos \(\beta\), baseado na matriz de sa√≠da \(Y\) e na matriz de pesos aleat√≥rios \(W\), por meio da resolu√ß√£o de um sistema linear.</p>

<h2 id="derterminando-a-matriz-h">Derterminando a matriz H</h2>

<p>Ap√≥s inicializar a matriz de pesos aleatoriamente, o pr√≥ximo passo √© determinar a matriz \(H\) da seguinte maneira:</p>

\[\mathbf{H}^i = [x^i_1, \cdots, x^i_m, 1] \times
\begin{bmatrix}
w_{11}&amp; \cdots &amp; w_{1d} \\
\vdots &amp; \ddots &amp; \vdots \\
w_{m1}&amp; \cdots &amp; w_{md} \\
b_1 &amp; \cdots &amp; b_d
\end{bmatrix}
\Rightarrow
\mathbf{H} = \begin{bmatrix}
f(H^1)\\
f(H^2) \\
\vdots\\
f(H^N)
\end{bmatrix}_{N \times d}

\tag{3}\]

<p>Na qual a fun√ß√£o \(f(.)\) √© a fun√ß√£o de transfer√™ncia (pode ser uma sigmoid, por exemplo) da camada e \(i = \{1, \cdots, N\}\), sendo \(N\) o n√∫mero de amostras do conjunto de treinamento. Portanto, a matriz \(\mathbf{H}\) armazena o resultado de todos os neur√¥nios da camada oculta obtidos a partir da multiplica√ß√£o entre \(\mathbf{X}\) e \(\mathbf{W}\). Observe que essa multiplica√ß√£o nada mais √© do que a forma matricial do neur√¥nio perceptron, que j√° foi descrito no <a href="/artigos/redes-neurais-artificiais/">post de redes neurais</a>. Al√©m disso, observe que os <em>bias</em> foram adicionados na √∫ltima linha da matriz de pesos e, por conta disso, no array de entrada \(X\), √© adicionado uma coluna de 1. Isso nada mais √© do que uma jogada matem√°tica que agiliza a computa√ß√£o e facilita a implementa√ß√£o. Caso m√£o tenha entendido, retire os bias da matriz de peso e a coluna de 1 do array de entrada, efetue a multiplica√ß√£o \(X \times W\) e na sequ√™ncia some com \(b\). O resultado √© o mesmo!</p>

<h2 id="determinando-a-matriz-de-features-beta">Determinando a matriz de <em>features</em> \(\beta\)</h2>

<p>Uma vez determinada a matriz \(H\), para se obter os pesos da matriz \(\boldsymbol{\beta}\) deve ser solucionado o seguinte sistema linear:</p>

\[\mathbf{H} \boldsymbol{\beta} = \mathbf{Y} \rightarrow \boldsymbol{\beta} = \mathbf{H}^\dagger \mathbf{Y}

\tag{4}\]

<p>Na qual \(\mathbf{H}^\dagger\) √© a inversa generalizada de Moore‚ÄìPenrose <a href="#serre">[3]</a> da matriz \(\mathbf{H}\). Caso fosse utilizado a inversa padr√£o, o algoritmos seria limitado a problemas que essa inversa existisse. A inversa generalizada ‚Äòafrouxa‚Äô algumas exig√™ncias da inversa tradicional, como por exemplo, a matriz n√£o precisa ser quadrada. Para mais informa√ß√µes, consulte <a href="#serre">[3]</a>. (Para quem usa Python+Numpy e/ou MATLAB, √© o comando pinv(), j√° implementado nas bibliotecas de algebra linear).</p>

<p>Por fim, vale a pena ressaltar que a matriz \(\beta\) √© conhecida como matriz de <em>features</em> porque ela guarda as informa√ß√µes extra√≠das pela rede. Como a ELM possui apenas uma camada oculta, essa √© a √∫nica informa√ß√£o ‚Äú√∫til‚Äù que a rede armazena, uma vez que os pesos da camada de entrada √© calculado de maneira aleat√≥ria.</p>

<h2 id="limita√ß√µes-e-outras-vers√µes-da-elm">Limita√ß√µes e outras vers√µes da ELM</h2>

<p>Bom, realizado os passos acima a rede est√° treinada e pode ser executada. Observe, que devido ao fato do treinamento da ELM ser executado de forma anal√≠tica, o mesmo √© realizado de maneira mais r√°pida do que um m√©todo iterativo <a href="#huang">[2]</a>. Todavia a abordagem possui suas fraquezas. A primeira delas √© relacionada a inicializa√ß√£o aleat√≥ria dos pesos da matriz \(\mathbf{W}\). Pode ocorrer dos valores obtidos para \(\mathbf{W}\) desencadear, ao fim do processo, em uma matriz \(\boldsymbol{\beta}\) que proporcione um resultado final ruim. Outro ponto √© que a matriz \(H\) pode ser singular, ou seja, n√£o √© poss√≠vel encontrar uma matriz inversa para a mesma. Por conta disso, Huang et al. <a href="#huang">[2]</a> prop√µe algumas t√©cnicas para que isso seja evitado. Al√©m disso, o c√°lculo da inversa generalizada pode ser custoso se a rede possui muitos neur√¥nios na camada oculta e muitas entradas na base de dados. Por conta disso, existe uma vers√£o do algoritmo chamado <em>Online Sequential</em> ELM. Em breve farei um post discutindo esse algoritmo, aguarde.</p>

<p>Existem diversos trabalhos que visam otimizar a escolha dos valores de \(\mathbf{W}\) por meio do uso de algoritmos evolutivos como o <a href="/algoritmos/o-algoritmo-genetico/">GA</a> <a href="#han">[4]</a>, por exemplo. Por fim, a rede realmente obt√©m bons resultados e pode ser milhares de vezes mais r√°pida do que uma rede neural tradicional. Obviamente, isso sempre vai depender do seu problema.</p>

<h1 id="c√≥digo-em-python-e-matlab">C√≥digo em Python e MATLAB</h1>

<p>Para voc√™ testar o algoritmo, deixo linjado o c√≥digo do mesmo em Python e MATLAB. Caso encontre algum bug ou tenha alguma sugest√£o, sinta-se livre para entrar em contato. Fa√ßa bom uso!</p>

<ul>
  <li><a href="https://github.com/paaatcha/ELM">C√≥digo ELM em python</a></li>
  <li><a href="https://github.com/paaatcha/Agregacao/tree/master/ELM">C√≥digo ELM em MATLAB</a></li>
</ul>

<h1 id="refer√™ncias">Refer√™ncias</h1>

<p><a name="huang">[1]</a> Huang, G.-B.; Zhu, Q.-Y.; Siew, C.-K. Extreme learning machine: a new learning scheme of feedforward neural networks. IEEE International Joint Conference on Neural Networks, v. 2, p. 985‚Äì990, 2004.</p>

<p><a name="huang2006">[2]</a> Huang, G.-B.; Zhu, Q.-Y.; Siew, C.-K. Extreme learning machine: theory and applications. Neurocomputing, v. 70, n. 1, p. 489‚Äì501, 2006</p>

<p><a name="serre">[3]</a> Serre, D. Matrices: theory and applications. New York: Springer, 2002.</p>

<p><a name="han">[4]</a> Han, F.; Yao, H.-F.; Ling, Q.-H. An improved evolutionary extreme learning machine based on particle swarm optimization. Neurocomputing, v. 116, p. 87‚Äì93, 2013.</p>
:ET