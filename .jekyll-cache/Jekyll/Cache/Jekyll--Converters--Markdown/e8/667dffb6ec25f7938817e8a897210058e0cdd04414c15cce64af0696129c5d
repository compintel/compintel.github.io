I"ëy<p>Esta √© a √∫ltima parte da nossa aula 1 do curso de <em>machine learning</em>. Nesta aula vamos abordar os conceitos b√°sicos e necess√°rios sobre probabilidade e teoria da informa√ß√£o. Um dos pilares de <em>machine learning</em> √© probabilidade. Os algoritmos s√£o, em sua maior parte, estoc√°sticos e fortemente baseados em estat√≠stica. Portanto, essa aula tamb√©m √© muito importante para se iniciar no assunto. Essa aula √© fortemente baseada em tr√™s livros que deixo linkado nas refer√™ncias. Lembrando, que isso √© uma revis√£o do b√°sico necess√°rio. Para mais informa√ß√µes, recorra as <a href="#refs">refer√™ncias</a>.</p>

<h1 id="introdu√ß√£o">Introdu√ß√£o</h1>
<p>De maneira geral, a teoria de probabilidade √© um conjunto de regras, axiomas e teoremas matem√°ticos para representar incerteza. Fen√¥menos incertos ocorrem a todo momento. O mundo real √© incerto. Por exemplo, a <strong>previs√£o</strong> do tempo √© um modelo probabil√≠stico. Existe uma probabilidade de determinado evento ocorrer, por exemplo, chover, e a teoria da probabilidade nos oferece um ferramental para modelar esses eventos incertos. Por outro lado, a teoria da informa√ß√£o nos permite a quantificar essas incertezas. Por conta disso, ambas teorias s√£o important√≠ssimas em diversas √°reas da ci√™ncia e engenharia. Em <em>machine learning</em> n√£o √© diferente, uma vez que os algoritmos aprendem a partir dos dados, certamente os modelos tamb√©m ser√£o incertos, uma vez que √© extramente complicado (para n√£o dizer imposs√≠vel) modelar o mundo real perfeitamente. Neste caso, a incerteza √© gerada pois o modelo √© <strong>incompleto</strong>. Um exemplos simples √© quando discretizamos um sinal cont√≠nuo. Ao fazer isso, estamos <strong>descartando</strong> informa√ß√µes, o que gera imprecis√£o.</p>

<p>√â importante ressaltar que o fato de um modelo ser probabilistico definitivamente n√£o quer dizer que ele √© ruim. Na verdade, em muitos casos √© mais pr√°tico ter um modelo simples mas incerto do que uma regra extramamente complexa e certa. Tomando o exemplo utilizado em <a href="#goodfellow">[1]</a>, imagine a seguinte regra: <em>a maioria dos p√°ssaros voam</em>. Ela √© simples e serve para uma gama enorme de pass√°ros. Mas ela incerta, pois sabemos que existe exce√ß√µes. Uma op√ß√£o a mesma, seria: <em>p√°ssaros voam, exceto os que ainda n√£o aprenderam, os machucados, Ema, Avestruz etc.</em> Essa seria uma regra certa, mas extremamente complexa de modelar e manter.</p>

<h2 id="probabilidade-frequentista-e-bayesiana">Probabilidade frequentista e Bayesiana</h2>
<p>A teoria da probabilidade foi originalmente criada para analizar frequ√™ncia de eventos. Por exemplo, o famoso exemplo de lan√ßar uma moeda para obter cara ou coroa. √â claro que a probilidade de cada evento √© 50%. Isso significa que se a repetirmos esse experimento, ou seja, jogar uma moeda para cima, uma quantidade de vezes muito grande a quantidade de vezes que se obtem cara ou coroa vai convergir para o valor de 50%. Esse tipo de probabilidade √© conhecida como <strong>frequentista.</strong></p>

<p>Por outro lado, existem certos problemas que essa caractr√≠stica de repeti√ß√£o n√£o ocorre. Se voc√™ vai em um m√©dico e ele diz que voc√™ tem 60% de chance de ter uma doen√ßa X, essa probabilidade <strong>significa algo diferente</strong>. N√£o √© poss√≠vel fazer varias replicas de sua pessoa para repetir os seus sintomas e verificar que voc√™ est√° com X. Neste caso, a probabilidade que o m√©dico informa √© um <strong>grau de confian√ßa</strong>, na qual 100% representa certeza que voc√™ est√° com a doen√ßa X e 0% que n√£o est√°. Esse tipo de probabilidade √© conhecida como <strong>Bayesiana</strong>.</p>

<h1 id="probabilidade">Probabilidade</h1>
<p>Vamos come√ßar definido probabilidade e tr√™s conceitos b√°sicos que ser√£o muito importantes ao longo do nosso curso: probabilidade condicional, conjunta e marginal.</p>

<h2 id="defini√ß√£o">Defini√ß√£o</h2>
<p>Para definir probabilidade, vamos considerar um experimento com um conjunto de resultados \(\Omega\). Esse experimento pode ser o famoso exemplo de rolar um dado (um padr√£o, n√£o um de RPG rs), que gera um conjunto \(\Omega\) com seis resultados poss√≠veis, \(\{\omega_1, \omega_2, \omega_3, \omega_4, \omega_5, \omega_6 \}\), ou seja, valores de 1 a 6. Neste caso o nosso <strong>evento</strong> de lan√ßar o dado resulta em 6 <strong>estados</strong>. Sendo assim, a probabilidade √© uma <strong>medida</strong> de que leva em considera√ß√£o as <strong>possibilidades</strong> de um determinado estado ocorrer em um evento (perceba que essa √© uma vis√£o frequentista, ainda vamos chegar na Bayesiana). Para ser considerada uma medida \(p\) ser considerada uma probabilidade, ela tem que satisfazer as seguintes condi√ß√µes:</p>

<ol>
  <li>\(p(\Omega) = 1\), em outras palavras, \(\sum_{i} \omega_i = 1\)</li>
  <li>\(p(\emptyset) = 0\), isto √©, um evento imposs√≠vel tem probabilidade zero</li>
  <li>Sendo \(\omega_A, \omega_B \in \Omega\) ent√£o: \(p(\omega_A \cup \omega_B) + p(\omega_A \cap \omega_B) = p(\omega_A) + p(\omega_B)\)</li>
  <li>Sendo \(\omega_A, \omega_B \in \Omega\) ent√£o: se \(\omega_A \subset \omega_B\) ent√£o \(p(\omega_A) \leq p(\omega_B)\)</li>
</ol>

<p>As duas primeiras propriedades s√£o as defini√ß√µes dos estados certo e imposs√≠vel. A terceira √© a propriedade de aditividade. N√£o vou entrar a fundo em teoria da medida, mas de maneira geral, a probabilidade √© uma medida aditiva. Isso que dizer que a <em>contribui√ß√£o</em> em valor dado por dois estadps √© a soma dos mesmos (em medidas n√£o aditivas pode ocorrer que a contribui√ß√£o de A e B resulte em um valor maior ou menor do que soma dos dois). A quarta e √∫ltima √© a propriedade de ordem. Considerando nosso exemplo de rolar um dado, imagine que adicionamos o estado \(\omega_{&lt;3}\), que √© o resultado: <em>obter um n√∫mero menor que 3</em>. O estado \(\omega_{1}\) j√° est√° contido em \(\omega_{3}\), logo sua probabilidade n√£o pode ser maior que a deste.</p>

<h2 id="probabilidade-condicional">Probabilidade condicional</h2>
<p>Muita das vezes queremos calcular a probabilidade de um evento sabendo que outro evento j√° ocorreu. Neste caso, temos uma informa√ß√£o pr√©via. Esse tipo de probabilidade √© conhecida como <strong>probabilidade condicional</strong>. Portanto, considerando \(\omega_A, \omega_B \in \Omega\), temos que a probabilidade de \(\omega_A\) dado que \(\omega_B\) j√° aconteceu, √© definido por:</p>

\[p(\omega_A \mid \omega_B) = \frac{p(\omega_A, \omega_B)}{p(\omega_B)}
\tag{1}\]

<p>A probabilidade \(p(\omega_A, \omega_B)\) √© conhecida como probabilidade conjunta e vamos tratar dela na sequ√™ncia. De maneira geral, ela √© a probabilidade de ambos \(\omega_A\) e \(\omega_B\) ocorrerem. Observe que a probabilidade condicional s√≥ existe se \(p(\omega_B) &gt; 0\). Se pensarmos bem, se \(\omega_B\) nunca ocorre, n√£o faz sentido calcular nada condicionado a ele.</p>

<h2 id="probabilidade-conjunta">Probabilidade conjunta</h2>
<p>As vezes temos interesse em saber a probabilidade de mais de uma estado ocorrer ao mesmo tempo. Podemos considerar 1 ou mais eventos; Por exemplo, considere que rolemos um dado e em seguida lan√ßamos uma moeda. Qual a probabilidade de obtermos coroa e 4? Nesse caso, a probabilidade seria:</p>

\[p(\omega_{co}, \omega_4) = p(\omega_{co})p(\omega_4)
\tag{2}\]

<p>Por√©m, a equa√ß√£o 2 s√≥ √© v√°lida pois os dois eventos s√£o independentes, ou seja, o fato de obter cara ou coroa <strong>n√£o interfere</strong> no valor obtido do dado. Observe que o c√°lculo dessa probabilidade nada mais √© do que a manipula√ß√£o da equa√ß√£o 1. Por√©m, \(p(\omega_{co} \mid \omega_4) = p(\omega_{co})\) uma vez que os eventos s√£o independentes. Sendo assim, a generaliza√ß√£o da probabilidade conjunta de dois estados √© descrita a seguir:</p>

\[p(\omega_A, \omega_B) = p(\omega_A \mid \omega_B) p(\omega_B)
\tag{3}\]

<p>A equa√ß√£o 3 reflete a probabilidade conjunta para dois eventos, por√©m, podemos generalizar para \(n\) utilizando a <strong>regra da cadeia de probabilidades condicionais</strong>:</p>

\[p(\omega^n, \cdots, \omega^1) = \prod_{k=1}^n p(\omega^k \mid \omega^{1}, \cdots, \omega^{k-1})
\tag{4}\]

<p>Por exemplo, para tr√™s eventos, temos:</p>

\[p(\omega^3, \omega^2, \omega^1) = p(\omega^3 | \omega^2, \omega^1) p(\omega^2, \omega^1) \\
p(\omega^3, \omega^2, \omega^1) = p(\omega^3 | \omega^2, \omega^1) p(\omega^2 \mid \omega^1) p(\omega_1)\]

<h2 id="probabilidade-marginal">Probabilidade marginal</h2>
<p>Imagine que temos dois eventos \(A\) e \(B\). Imagine tamb√©m que sabemos as probabilidades dos estados combinados, mas queremos saber a probabilidade de um estado espec√≠fico. Esta opera√ß√£o √© conhecida como probabilidade marginal e √© definida fizando o estado que queremos e variando os demais, como descrito a seguir:</p>

\[p(\omega_a) = \sum_{\Omega_B} p(\omega_a, \omega_b)
\tag{5}\]

<p>Tentado explicar de maneira did√°tica a equa√ß√£o acima, imagina o experimento de rolar o dado e lan√ßar uma moeda. Esse experimento vai gerar um conjunto \(\Omega\) que √© formado por todos os estados de ambos os eventos: \(\Omega = \{ (\omega_{co}, \omega_1), (\omega_{ca}, \omega_1), \cdots, (\omega_{ca}, \omega_6), (\omega_{co}, \omega_6)  \}\). Cada um destes estados possui uma probabilidade. Agora imagina que desejamos calcular a probabilidade de se obter cara, ou seja, \(p(\omega_{ca})\). Nesse caso, somamos todas as probabilides que cont√©m esse estado: \(p(\omega_{ca}) = p(\omega_{ca}, \omega_1) + \cdots + p(\omega_{ca}, \omega_6)\). √â exatamente isso que a equa√ß√£o 5 faz, fixa o estado \(a\) e varia o \(b\).</p>

<h1 id="distribui√ß√µes-de-probabilidade">Distribui√ß√µes de probabilidade</h1>
<p>At√© o momento, apresentamos as defini√ß√µes b√°sicas de probabilidade pensando em enventos simples, como lan√ßar uma moeda. Acontece que a teoria da probabilidade √© utilizada para modelar eventos muitos mais complexos. V√°rios problemas podem ser modelados ou aproximados por uma <strong>distribui√ß√£o de probabilidade</strong> e √© disso que vamos tratar nessa se√ß√£o.</p>

<h2 id="vari√°veis-aleat√≥rias">Vari√°veis aleat√≥rias</h2>
<p>Para definir uma vari√°vel aleat√≥ria, vamos considerar um experimento com um conjunto de resultados \(\Omega\). Esse experimento pode ser, novamente, lan√ßar uma moeda, que gera um conjunto \(\Omega\) com dois resultados, \(\{\omega_{ca}, \omega_{co} \}\), ou seja, cara ou coroa. Sendo assim uma <strong>vari√°vel aleat√≥ria</strong> nada mais √© do que uma vari√°vel que pode tomar um dos valores do conjunto \(\Omega\). Neste exemplo, a vari√°vel aleat√≥ria √© <em>lan√ßar um dado</em> e ela pode assumir os valores \(\omega_{ca}\) ou \(\omega_{co}\). Ela √© aleat√≥ria pois n√£o sabemos o seu resultado, mas podemos ter uma probabilidade dele acontecer.</p>

<p>Em outras palavras, uma vari√°vel aleat√≥ria √© uma descri√ß√£o de todos os estados poss√≠veis de um evento. Essa vari√°vel pode ser modelada/aproximada por uma <strong>distribui√ß√£o de probabilidade</strong>, que basicamente ser√° uma fun√ß√£o que especifica o qu√£o prov√°vel √© cada um dos estados. Al√©m disso, vari√°veis aleat√≥rias podem ser discretas ou cont√≠nuas. Em resumo, eventos discretos possuem um n√∫mero finito de estados. Por outro lado, eventos cont√≠nuos s√£o representados por n√∫meros reais, portanto, assumem valores infinitos. Isso vai afetar na sua distribui√ß√£o, como vamos ver na sequ√™ncia.</p>

<h2 id="histogramas">Histogramas</h2>
<p>Umas das melhores formas de descrever uma vari√°vel √© descrevendo sua frequ√™ncia no conjunto de dados. Essa descri√ß√£o √© conhecida como <strong>distribui√ß√£o da vari√°vel</strong>. A maneira mais comum de fazer isso √© por meio de um <strong>histograma</strong>, que nada mais √© do que um gr√°fico que mostra a frequ√™ncia de cada valor. A Figura 1 exemplifica esse conceito. Imagine uma turma e desejamos calcular plotar o histograma de acordo com a idade. No eixo <em>y</em> temos a frequ√™ncia e no <em>x</em> as idades. Sendo assim, sabemos que temos mais alunos com 19 e 20 anos nesta turma.</p>

<figure style="width: 490px; height: 400px;" class="align-center">
  
  <img src="http://0.0.0.0:4000/assets/img/cursoML/aula1-3/hist_idade.png" alt="" />

  <figcaption style="text-align: center;">
    Figura 1: exemplo de um histograma de idades de uma turma
  </figcaption>

</figure>

<p>Como disse, o histograma √© muito comum. Mas pode surgir a seguinte pergunta: qual a probabilidade de um aluno ter 19 anos nesta turma? Como temos as frequ√™ncias, podemos simplesmente <em>transformar</em> esses valores em probabilidade dividindo todos pelo somat√≥rio total de todas as frequ√™ncias. Dai teremos o gr√°fico da Figura 2.</p>

<figure style="width: 490px; height: 400px;" class="align-center">
  
  <img src="http://0.0.0.0:4000/assets/img/cursoML/aula1-3/prob_idade.png" alt="" />

  <figcaption style="text-align: center;">
    Figura 2: exemplo de uma distribui√ß√£o de probabilidade de idades
  </figcaption>

</figure>

<p>Neste caso, temos uma fun√ß√£o de densidade de probabilidade (FDP ou PMF do ingl√™s). Idade, nada mais √© do que nossa nossa vari√°vel aleat√≥ria na qual queremos entend√™-la por meio de uma <strong>distribui√ß√£o de probabilidade.</strong></p>

<p>Uma <strong>distribui√ß√£o de probabilidade</strong> √© uma descri√ß√£o de qu√£o prov√°vel uma ou mais vari√°veis aleat√≥rias de tomar um determinado estado. No nosso exemplo, os estados s√£o as idades descritas no eixo <em>x</em> dos gr√°ficos anteriores. Essa distribui√ß√£o √© descrita por uma fun√ß√£o que recebe como par√¢metro o valor da vari√°vel e retorna a probabilidade da mesma ocorer. Por√©m, pode ser bem dif√≠cil encontrar essa fun√ß√£o de maneira exata. Para facilitar nossa vida, existem diversas distribui√ß√µes conhecidas que podemos aproximar para o nosso problema (vamos abordar isso nas pr√≥ximas se√ß√µes). Todavia, a maneira que uma distribui√ß√£o √© descrita depende da vari√°vel ser cont√≠nua ou discreta. O exemplo anterior √© para uma fun√ß√£o discreta, ou seja, temos um conjunto finito de idades (no caso 6) pr√©-definidas.</p>

<h2 id="vari√°veis-aleat√≥rias-discretas-e-fdps">Vari√°veis aleat√≥rias discretas e FDPs</h2>
<p>Como j√° mostramos no exemplo anterior, para vari√°veis discretas, a sua distribui√ß√£o de probabilidade ser√° descrita por uma fun√ß√£o de densidade de probabilidade (FDP). Normalmente, a nota√ß√£o utilizada para uma FDP √© a letra mai√∫scula \(P\). Al√©m disso, quando encontramos \(P(\textrm{x})\) e \(P(\textrm{y})\), normalmente, √© uma distribui√ß√£o para \(\textrm{x}\) e outra para \(\textrm{y}\). Essa nota√ß√£o √© confusa, mas √© a maneira que √© utilizada (n√£o me culpem).</p>

<p>Como j√° sabemos, a FDP vai mapear o estado de uma vari√°vel aleat√≥ria para uma probabilidade da vari√°vel assumir aquele estado. Sendo assim, outra fonte de confus√£o nas nota√ß√µes √© que \(P(\textrm{x})\) mapeia a probabilidade de \(\textrm{x} = x\), ou seja, da vari√°vel aleat√≥ria \(\textrm{x}\) assumir um estado \(x\). Por isso, tamb√©m podemos encontrar a seguinte nota√ß√£o \(P(\textrm{x} = x)\)</p>

<p>Uma distribui√ß√£o pode ser com respeito a uma ou mais vari√°veis. No caso de duas, ela √© conhecida como <strong>distribui√ß√£o de probabilidade conjunta</strong>. Neste caso, teremos uma distribui√ß√£o \(P(\textrm{x} = x, \textrm{y} = y)\), ou seja, a probabildiade de \(\textrm{x}\) assumir um estado \(x\) <strong>e</strong> \(\textrm{y}\) assumir um estado \(y\). A nota√ß√£o simplificada segue o mesmo padr√£o anterior \(P(x,y)\).</p>

<p>Para uma fun√ß√£o ser considerada uma FDP (ok, controle a 5¬™ s√©rie que existe dentro de voc√™ rs) de uma vari√°vel aleat√≥ria \(\textrm{x}\), ela precisa respeitar as seguintes propriedades:</p>
<ul>
  <li>
    <p>O dom√≠nio de \(P\) deve ser o conjunto de todos poss√≠ves estados de \(\textrm{x}\).</p>
  </li>
  <li>
    <p>\(\forall x \in \textrm{x}, 0 \leq P(x) \leq 1\), al√©m disso, um evento imposs√≠vel dever ter probabilidade \(0\) e um evento garantido \(1\).</p>
  </li>
  <li>
    <p>\(\sum_{x\in \textrm{x}} P(x) = 1\). Essa √© a propriedade de normaliza√ß√£o. N√≥s fizemos isso no exemplo do histograma ao dividir as frequ√™ncias pelo somat√≥rio total.</p>
  </li>
</ul>

<h3 id="exemplo-de-fdp">Exemplo de FDP</h3>
<p>Um exemplo simples de uma FDP de uma vari√°vel aleat√≥ria \(\textrm{x}\) com \(k\) estados diferentes pode ser descrita pela seguinte equa√ß√£o:</p>

\[P(\textrm{x} = x_i) = \frac{1}{k}
\tag{6}\]

<p>Essa distribui√ß√£o √© conhecida como <strong>distribui√ß√£o uniforme</strong>. Isso significa que qualquer estado \(k\) tem a mesma probabilidade de ocorrer. O nosso exemplo de rolar um dado √© descrito por essa distribui√ß√£o, ou seja, obter os valores de 1 a 6 tem a mesma probabilidade. Neste caso, dizemos que a vari√°vel aleat√≥ria <em>lan√ßar um dado</em> (\(A\)) segue a distribui√ß√£o uniforme (\(U\)).</p>

<p>Por fim, √© muito f√°cil de provar que todas as propriedades descritas acima s√£o satisfeitas, fazendo com que essa simples fun√ß√£o se torne uma FDP.</p>

<h2 id="vari√°veis-aleat√≥rias-discretas-e-a-fun√ß√£o-de-distribui√ß√£o-acumulada-fda">Vari√°veis aleat√≥rias discretas e a fun√ß√£o de distribui√ß√£o acumulada (FDA)</h2>
<p>Quando estamos trabalhando com vari√°veis cont√≠nuas a FDP j√° n√£o √© mais √∫til e o motivo √© que mensurar a probabilidade de um √∫nico ponto cont√≠nuo pode ser complicado. Por isso √© introduzida a <strong>fun√ß√£o de distribui√ß√£o acumulada</strong> (FDA). Neste caso, \(p(x)\) n√£o ser√° utilizado para obter a probabilidade de um ponto espec√≠fico, mas sim de um intervalo. Matem√°ticamente falando, essa probabilidade estar√° em uma regi√£o inifitesimal e ser√° calculado por uma integral.</p>

<p>Das regras para ser uma FDP, a √∫nica que se altera para FDA √© a √∫ltima. O somat√≥rio √© substitu√≠do por uma integral, tomando a forma \(\int p(x)dx = 1\).</p>

<p>Como n√£o estamos mais interessados em um ponto espec√≠fico e sim em um intervalo, calculamos a √°rea abaixo da curva do mesmo, ou seja, integramos a distribui√ß√£o naquele intervalo:</p>

\[p(x \in [a,b]) = \int_a^b p(x)dx
\tag{7}\]

<p>Neste curso vamos focar apenas nas distribui√ß√µes discretas. No fundo, a distribui√ß√£o cont√≠nua √© a ‚Äúgeneraliza√ß√£o‚Äù da discreta. No mundo discreto uma integral nada mais √© do que um somat√≥rio. Como exemplo, voc√™ pode imaginar uma distribui√ß√£o uniforme para n√∫meros reais. Ela pode assumir infinitos valores.</p>

<h2 id="valor-esperado-vari√¢ncia-e-covari√¢ncia">Valor esperado, vari√¢ncia e covari√¢ncia</h2>
<h3 id="valor-esperado">Valor esperado</h3>
<p>Uma das opera√ß√µes mais comuns para vari√°veis aleat√≥rias √© o c√°lculo do valor esperado. Ele √© uma tentativa de prever ou resumir o comportamento de uma vari√°vel aleat√≥ria. Para exemplificar sua aplica√ß√£o, imagine que o tempo de entrega de uma encomenda segue uma distribui√ß√£o de probabilidade \(P\). Utilizando o valor esperado √© poss√≠vel estimar o temo m√©dio de entrega dessa encomenda.</p>

<p>A defini√ß√£o do valor esperado √© o seguinre: Seja \(\textrm{x}\) uma vari√°vel aleat√≥ria discreta o seu valor esperado √© descrito pela seguinte equa√ß√£o:</p>

\[E[x] = \sum_{i=1}^n x_iP(x_i)
\tag{8}\]

<p>Observe que se a distribui√ß√£o √© uniforme, o valor esperado √© o mesmo valor da m√©dia aritim√©tica de um conjunto de valores.</p>

<p>Essa mesma defini√ß√£o pode ser generalizada para uma fun√ß√£o que toma valores no espa√ßo amostral de \(\textrm{x}\):</p>

\[E[f(x)] = \sum_{i=1}^n f(x_i)P(x_i)
\tag{9}\]

<p>O valor esperado tem uma s√©rie de propriedades √∫teis e interessantes. Podemos destacar:</p>
<ol>
  <li>\(E\) de uma constante: \(E[\alpha] = \alpha\)</li>
  <li>\(E\) de uma constante mais uma vari√°vel aleat√≥ria: \(E[\alpha + \textrm{x}] = \alpha + E[\textrm{x}]\)</li>
  <li>\(E\) de uma constante vezes uma vari√°vel aleat√≥ria: \(E[\alpha \textrm{x}] = \alpha E[\textrm{x}]\)</li>
  <li>O valor esperado de uma combina√ß√£o linear de vari√°veis aleat√≥rias: \(E[\alpha \textrm{x} + \beta \textrm{y}] = \alpha E[\textrm{x}] + \beta E[\textrm{y}]\)</li>
</ol>

<p>Por fim, existem dois teoremas bem poderosos em rela√ß√£o ao valor esperado, s√£o eles:
<strong>Teorema 1:</strong> Sejam \(\textrm{x1}, \cdots, \textrm{x_n}\) vari√°veis aleat√≥rias, dependentes entre si ou n√£o, temos que:</p>

\[E[\textrm{x1}, \cdots, \textrm{x_n}] = E[\textrm{x1}] + \cdots + E[\textrm{x_n}]
\tag{10}\]

<p>Perceba, que o teorema √© basicamente uma generaliza√ß√£o da propriedade 4, mas para mais vari√°veis.</p>

<p><strong>Teorema 2:</strong> Sejam \(\textrm{x}_1\) e \(\textrm{x}_2\) dusa vari√°veis aleat√≥rias <strong>independentes</strong>, ent√£o:</p>

\[E[\textrm{x}_1 \textrm{x}_2] = E[\textrm{x}_1] E[\textrm{x}_2]
\tag{11}\]

<h3 id="vari√¢ncia">Vari√¢ncia</h3>
<p>A vari√¢ncia de uma vari√°vel aleat√≥ria √© uma medida que mostra o qu√£o <em>espalhada</em> essa vari√°vel est√°. Em outras palavras, o quanto os valores que ela assume se afastam do valor esperado. Seja \(\textrm{x}\) uma vari√°vel aleat√≥ria, sua vari√¢ncia √© descrita por:</p>

\[Var[\textrm{x}] = E[(\textrm{x} - E[\textrm{x}])^2]
\tag{12}\]

<p>Ou, atrav√©s de uma manipula√ß√£o alg√©brica:
\(Var[\textrm{x}] = E[\textrm{x}^2]- E[\textrm{x}]^2
\tag{13}\)</p>

<p>A vari√¢cia √© frequentemente denotada por \(\sigma^2\). O motivo √© que a raiz quadrada da vari√¢ncia √© uma medida conhecida como <strong>desvio padr√£o</strong>:</p>

\[\sigma = \sqrt{Var(\textrm{x})}
\tag{14}\]

<p>A vari√¢ncia n√£o segue as mesmas propriedades do valor esperado. Por√©m, se duas vari√°veis aleat√≥rias \(\textrm{x}_1\) e \(\textrm{x}_2\) s√£o <strong>independentes</strong>, ent√£o a vari√¢ncia da sua soma pode ser calculada da seguinte forma:</p>

\[Var(\textrm{x}_1 + \textrm{x}_2) = Var(\textrm{x}_1) Var(\textrm{x}_2)
\tag{15}\]

<h3 id="covari√¢ncia">Covari√¢ncia</h3>
<p>A covari√¢ncia √© uma medida de qu√£o linearmente relacionadas duas vari√°veis aleat√≥rias s√£o. Por exemplo, sejam duas vari√°veis aleat√≥rias \(\textrm{x}_1\) e \(\textrm{x}_2\), o que ocorre com  \(\textrm{x}_1\) se aumentarmos o valor de \(\textrm{x}_2\) ? Para isso, √© utilizada a covari√¢ncia, que √© definida pela seguinte equa√ß√£o:</p>

\[Cov(\textrm{x}_1, \textrm{x}_2) = E[(\textrm{x}_1 - E[\textrm{x}_1])(\textrm{x}_2 - E[\textrm{x}_2])]
\tag{16}\]

<p>Um valor alto positivo siginifica que ao aumetar uma vari√°vel a outra tamb√©m aumenta. Por outro lado, para um valor alto negativo, se aumentarmos uma a outra diminui. Por fim, um valor igual ou pr√≥ximo de zero significa que a as vari√°veis n√£o apresentam uma depend√™ncia linear. Nessa parte √© preciso aten√ß√£o! Isso n√£o significa que as vari√°veis s√£o independentes, pois a covari√¢ncia n√£o leva em considera√ß√£o rela√ß√µes n√£o lineares.</p>

<p>Uma medida que normaliza a vari√¢ncia no intervalo \([-1,1]\) √© a correla√ß√£o. Temos, por exemplo, a correla√ß√£o de Pearson, definida como:</p>

\[\rho = \frac{Cov(\textrm{x}_1, \textrm{x}_2)}{\sigma_{\textrm{x}_1}\sigma_{\textrm{x}_2}}
\tag{17}\]

<p>Neste caso, \(+1\) indica uma correla√ß√£o crescente perfeita, \(-1\) uma decrescente perfeita e \(0\) uma n√£o correla√ß√£o.</p>

<h2 id="principais-distribui√ß√µes-de-probabilidade">Principais distribui√ß√µes de probabilidade</h2>
<p>Existem diversas distribui√ß√µes de probabilidade que s√£o muito comuns de serem utilizadas para modelas problemas no mundo real. Nesta se√ß√£o, vamos introduzir apenas 3: Bernoulli, Gaussiana e Poisson. Por√©m, existem diversas outras como t-student, exponencial, Beta, Dirichlet etc. Obviamente, n√£o vamos abordar todas elas e caso voc√™ precise, recorra as refer√™ncias.</p>

<h3 id="distribui√ß√£o-de-bernoulli">Distribui√ß√£o de Bernoulli</h3>
<p>A distribui√ß√£o de Bernoulli √© uma das mais b√°sicas existentes, na qual a vari√°vel aleat√≥ria pode tomar apenas dois valores, \(0\) ou \(1\). Em outras palavras, ela √© <strong>bin√°ria.</strong> A distribui√ß√£o √© especificado por um √∫nico par√¢metro \(p\) e por conven√ß√£o a distribui√ß√£o retorna a probabilidade da vari√°vel aleat√≥ria ser igual a \(1\), ou seja, \(P(\textrm{x}=1)\).</p>

<p>A equa√ß√£o que descreve a distribui√ß√£o √© apresentada a seguir:</p>

\[P(\textrm{x}) = p^x(1-p)^{1-x}
\tag{18}\]

<p>Apenas para exemplificar podemos modelar a vari√°vel aleat√≥ria <em>cara ou coroa</em> utilizando Bernoulli. Neste caso, a probabilidade de se obter cara (1) ou coroa (0) pode ser equacionada por 18 quando \(p=0.5\), ou seja: \(P(\textrm{x}) = 0.5^x(0.5)^{1-x}\).</p>

<p>Obviamente podemos calular o valor esperado e vari√¢ncia da distribui√ß√£o, que ser√£o iguais a \(p\) e \(p(1-p)\), respectivamente.</p>

<h3 id="distribui√ß√£o-de-poisson">Distribui√ß√£o de Poisson</h3>
<p>Outra distribui√ß√£o famosa √© a de Poisson, que √© muito utilizada para modelas problemas que envolve uma s√©rie de eventos em um certo per√≠odo de tempo. Esses eventos devem possuir uma taxa m√©dia fixa de ocorr√™ncia e ocorrem independentemente do √∫ltimo evento ocorrido. Sendo \(\lambda\) a taxa m√©dia de ocorr√™ncia, a distribui√ß√£o e descrita da seguinte forma:</p>

\[P(\textrm{x}) = \frac{e^{-\lambda} \lambda^x}{x!}
\tag{19}\]

<p>Nesta distribui√ß√£o, tanto o valor esperado quanto a vari√¢ncia s√£o iguais a \(\lambda\). Dois exemplos de problemas que podem ser modelados por Poisson s√£o: batimentos card√≠acos e chamadas telef√¥nicas, ambas por unidades de tempo.</p>

<h3 id="distribui√ß√£o-gaussiana">Distribui√ß√£o Gaussiana</h3>
<p>A distribui√ß√£o Gaussina ou Normal √© uma das distribui√ß√µes mais utilizadas para modelas fen√¥menos naturais. Ela √© extremamente vers√°til e pode ser utilizada para uma grande variedade de contextos. Obviamente, ser√° muito utilizada em <em>machine learning</em>.</p>

<p>A Gaussiana √© especificada por dois par√¢metros, a m√©dia (\(\mu\)) e o desvio padr√£o (\(\sigma\)). Sua FDP √© descrita por:</p>

\[P(\textrm{x}) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
\tag{20}\]

<p>O gr√°fico dessa distribui√ß√£o √© ilustrado pela Figura 3:</p>

<figure style="width: 490px; height: 440px;" class="align-center">
  
  <img src="http://0.0.0.0:4000/assets/img/cursoML/aula1-3/gaussiana.png" alt="" />

  <figcaption style="text-align: center;">
    Figura 3: varia√ß√£o da forma da Gaussiana de acordo com a m√©dia e desvio padr√£o. Fonte: 
    <a href="#ieong">[2]</a> 
  </figcaption>

</figure>

<p>Como disse, a Gaussiana pode ser utilizada para modelar os mais diversos problemas, inclusive o nosso exemplo da se√ß√£o do histograma, ou seja, modelar a idade de alunos de uma turma. Se voc√™ observar bem o gr√°fico da Figura 2, ele se parece um pouco com a forma de uma Gaussiana. Para plotarmos, teriamos que calcular a m√©dia de todas as idades e o desvio padr√£o. A partir disso √© poss√≠vel modelas a equa√ß√£o 20. Vamos brincar um pouco com essa teoria quando formos trabalhar com o classificador chamado Naive Bayes. E por falar Bayes, ele ser√° o tema da √∫ltima se√ß√£o desta aula.</p>

<h1 id="o-teorema-de-bayes">O Teorema de Bayes</h1>
<p>O Teorema de Bayes √© um dos teoremas mais importantes da teoria da probabilidade. Em resumo, ele descreve a probabilidade de um determinado evento baseado em um conhecimento pr√©vio sobre o mesmo. Esse conhecimento √© conhecido como <em>priore</em>. O teorema mostra que a partir do momento que temos novas evid√™ncias alteramos a probabilidade final que √© chamada de <em>posteriori</em>.</p>

<p>Para chegarmos a equa√ß√£o do teorema, √© relativamente simples. Thomas Bayes, um matem√°tico e pastor Ingl√™s, observou algo interessante em rela√ß√£o a probabilidade conjunta. Considerando duas vari√°veis aleat√≥rias \(A\) e \(B\), de acordo com a equa√ß√£o 3, sabemos que sua probabilidade conjunta √© descrita por:</p>

\[p(A, B) = p(A \mid B) p(B)
\tag{21}\]

<p>Por√©m, \(p(A, B) = p(B, A)\). Sendo assim, Bayes manipulou a equa√ß√£o para obter:</p>

\[p(A \mid B) p(B) = p(B \mid A) p(A) \\
p(A \mid B) = \frac{p(B \mid A) p(A)}{p(B)} \\
\tag{22}\]

<p>Dessa maneira, temos o famoso teorema de Bayes. Eu pretendo fazer um post dedicado s√≥ a ele. Mas de maneira geral, o que o teorema diz √©: queremos estimar \(A\) dado que temos uma evid√™ncia \(B\) que influ√™ncia \(A\). A \(p(A)\) √© a distribui√ß√£o a <em>priori</em>, ou seja, o que eu sei sobre uma determinada situa√ß√£o antes de tomar conhecimento sobre \(B\). \(p(B \mid A)\) √© conhecido como <em>likelihood</em>. Esse √© um conceito abstrato para se explicar em um resumo, mas voc√™ pode entender como o modelo de uma distribui√ß√£o de acordo com alguns dados conhecidos. J√° \(p(B)\) √© um fator de normaliza√ß√£o e pode ser calculado a partidar da probabilidade marginal, j√° discutida nessa aula. Por fim, \(p(A \mid B)\) √© a distribui√ß√£o <em>posteriori</em>, que queremos descobrir.</p>

<p>Para tentar ilustrar essa ideia com um exemplo, imagine que voc√™ esteja viajando e veja um animal que te lembra um cachorro, mas tamb√©m pode ser outro animal, como um gato ou uma raposa. Mas voc√™ infere que, a probabilidade de ser um cachorro dado que estou numa estrada brasileira, √© bem alta. Por√©m, voc√™ recebe uma nova informa√ß√£o que faz voc√™ mudar essa certeza que tinha que era um cachorro. Voc√™ descobre que a 50m do local onde avistou o animal existe uma ONG protetora de gatos. Com isso, automaticamente, aumenta significativamente a probabilidade do animal ser um gato, <strong>por que voc√™ teve uma nova evid√™ncia</strong>. Logo, voc√™ alterou a <em>posteriori</em> de acordo com a <em>priori</em>.</p>

<p>Como disse, pretendo fazer um post bem detalhado sobre estimativa Bayesiana e vou refer√™nci√°-lo aqui quando estiver pronto. Por hora, considere que o teorema existe e que ele bem importante e muito utilizado na computa√ß√£o em geral.</p>

<h1 id="considera√ß√µes-finais">Considera√ß√µes finais</h1>
<p>Essa foi a nossa √∫ltima aula de conceitos b√°sicos. A teoria da probabilidade √© um dos pilares de <em>machine learning</em>. Voc√™ vai perceber que os algoritmos (n√£o todos, mas boa parte) s√£o estoc√°sticos, ou seja, utilizam da teoria de probabilidade para gerar uma sa√≠da probabil√≠stica. Voc√™ vai entender isso em breve (eu espero).</p>

<p>Ent√£o √© isso, a partir da pr√≥xima aula, entraremos de vez no nosso tema principal. Para voc√™ que achou chato at√© aqui, tavez voc√™ encontre mais divers√£o na sequ√™ncia deste curso. At√© a pr√≥xima.</p>

<p><a name="refs"></a></p>
<h1 id="principais-refer√™ncias">Principais refer√™ncias</h1>
<p><a name="goodfellow">[1]</a> <em>Deep learning</em> - Ian Goodfellow, Yoshua Bengio e Aaron Courville <a href="https://www.deeplearningbook.org/">link</a></p>

<p><a name="ieong">[2]</a> <em>Probability Theory Review for Machine Learning</em> - Samuel Ieong <a href="http://web.stanford.edu/class/ee269/probability.pdf">link</a></p>

<p><a name="Jaynes">[3]</a> Probability theory: The logic of science - Jaynes, Edwin T. Cambridge university press, 2003.</p>

:ET