I"ïq<h1 id="introdu√ß√£o">Introdu√ß√£o</h1>
<p>Esta √© parte 2 da aula 1 do curso de <em>machine learning</em>. Nesta aula vamos abordar os conceitos b√°sicos e necess√°rios sobre C√°lculo para se iniciar na √°rea. Este post ser√° basicamente um apanhando geral do livro de <a href="https://www.amazon.com.br/C%C3%A1lculo-1-James-Stewart/dp/8522112584">C√°lculo I e II do James Stewart</a>. Este foi o livro que aprendi c√°lculo e o considero como uma excelente fonte. Existem alguns PDFs dele dispon√≠veis por ai <del>que voc√™ encontra no google</del>, mas eu n√£o sei onde encontr√°-los.</p>

<p>Essa aula ser√° dividida em 4 partes. Na primeira, vamos fazer uma breve revis√£o de algumas fun√ß√µes. Na segunda, vamos abordar o conceito de limite. Na terceira o de derivada e na quarta Minimiza√ß√£o/Maximiza√ß√£o por derivadas, que √© conceito mais importante o nosso prop√≥sito, ou seja, <em>machine learning</em>. Se voc√™ se sente confort√°vel com os conceitos fundamentais, j√° v√° direto para a <a href="#parte4">Parte 4</a>. Novamente, isso √© uma aula de revis√£o. Para saber mais recorra as refer√™ncias fornecidas.</p>

<h1 id="parte-1---preliminares">Parte 1 - Preliminares</h1>
<p>Antes de falar sobre limite, vamos abordar alguns conceitos que ser√£o √∫teis daqui pra frente. Vou tentar ser o mais breve poss√≠vel.</p>

<h2 id="fun√ß√£o-inversa">Fun√ß√£o inversa</h2>
<p>Dado uma fun√ß√£o \(g\), a fun√ß√£o \(f\) √© definida como sua inversa se:</p>

\[g(f(x)) = x  \textrm{ e } f(g(y)) = y \textrm{, }\forall x,y \textrm{ em que } g(y) \textrm{ e } f(x) \textrm{ existem}
\tag{1}\]

<p>A nota√ß√£o para a essa fun√ß√£o √© \(g = f^{-1}\). Por√©m, cuidado para n√£o confundir com \(a^{-1} = \frac{1}{a}\).</p>

<p>Calcular uma inversa √© bem simples. Basta igualar uma fun√ß√£o \(f(x) = y\) e isolar \(x\). Por exemplo, considere \(f(x) = 3 + \frac{1}{x}\). Para encontrar sua inversa fazemos \(f(x) = y\) e isolamos \(x\) da seguinte maneira:</p>

\[3 + \frac{1}{x} = y \\
x = \frac{1}{y-3}\]

<p>Dessa forma, \(f^{-1} = \frac{1}{y-3}\).</p>

<h2 id="fun√ß√£o-exponencial">Fun√ß√£o exponencial</h2>
<p>Seja \(a\) um n√∫mero positivo, fixo, chamado de base, sua exponencial √© definida como:</p>

\[a^n = a \times a \times \cdots \times a \textrm{ (n vezes) }
\tag{2}\]

<p>Algumas propriedades interessantes:</p>
<ul>
  <li>P1: \(a^m a^n = a^{m+n}\)</li>
  <li>P2: \((a^m)^n = a^{mn}\)</li>
  <li>P3: \((ab)^n = a^{n}b^n\)</li>
</ul>

<p>Al√©m disso, para que a fun√ß√£o seja definida para todos os n√∫meros reais, existem as seguintes defini√ß√µes:</p>

<ul>
  <li>D1: \(a^0 = 1\)</li>
  <li>D2: \(a^{-n} = \frac{1}{a^n}\)</li>
</ul>

<p>O gr√°fico b√°sico de uma exponencial √© exemplificado na Figura 1.</p>

<figure style="width: 490px; height: 400px;" class="align-center">
  
  <img src="http://0.0.0.0:4000/assets/img/cursoML/aula1-2/exp.png" alt="" />

  <figcaption style="text-align: center;">
    Figura 1: Gr√°fico b√°sico de uma exponencial. A medida que x cresce, sua imagem y √© cada vez mais amplificada. Essa √© a caracteristica b√°sica de uma exponencial. <a href="https://pt.wikipedia.org/wiki/Fun%C3%A7%C3%A3o_exponencial" target="blank"> Fonte </a>
  </figcaption>

</figure>

<h2 id="fun√ß√£o-logaritimica">Fun√ß√£o logaritimica</h2>
<p>Basicamente, o logaritmo √© a fun√ß√£o inversa da exponencial. Sendo \(a\) a base, um logartimo √© definido como:
\(\log_a x = z \Leftrightarrow a^z = x
\tag{3}\)</p>

<p>Como por defini√ß√£o \(a^0 = 1\), temos que \(\log_a 1 = 0\) e como \(a^1 = a\), temos que \(\log_a a = 1\). Outro detalhe importante √© que n√£o existe solu√ß√£o para equa√ß√£o \(a^z = 0\). Dessa forma, \(\log_a 0\) <strong>n√£o √© definido</strong>.</p>

<p>Como veremos mais para frente, o logaritmo √© importante para <em>machine learning</em> uma vez que suas propriedades auxiliam nas derivadas de algumas fun√ß√µes. Ent√£o, vamos as principais propriedades:</p>

<ul>
  <li>P1: \(\log_a (xy) = \log_a x + \log_a y\)</li>
  <li>P2: \(\log_a x^y = y \log_a x\)</li>
  <li>P3: \(\log_a \frac{x}{y} = \log_a x - \log_a y\)</li>
</ul>

<p>Al√©m das tr√™s propropriedades acima, outra muito importante √© a <strong>mudan√ßa de base</strong>:</p>

\[\log_b x = \frac{\log_a x}{\log_a b}
\tag{4}\]

<p>O gr√°fico b√°sico de um logaritmo √© mostrado na Figura 2.</p>

<figure style="width: 490px; height: 450px;" class="align-center">
  
  <img src="http://0.0.0.0:4000/assets/img/cursoML/aula1-2/log.png" alt="" />

  <figcaption style="text-align: center;">
    Figura 2: Gr√°fico b√°sico de um logaritmo. Observe que a curva n√£o toca o eixo y para x=0. Como a fun√ß√£o n√£o √© definida ali, o que temos √© uma ass√≠ntota. <a href="https://pt.wikipedia.org/wiki/Logaritmo" target="blank"> Fonte </a>
  </figcaption>

</figure>

<h3 id="logaritmo-neperiano">Logaritmo neperiano</h3>
<p>Quando a base do logaritmo √© igual a \(e = 2.718\cdots\), o logaratimo recebe o nome de neperiano ou natural. Assim como \(\pi\), \(e\) √© uma constante fundamental da matem√°tica. Dessa forma temos duas defini√ß√µes:</p>

<ul>
  <li>A exponencial de \(e\) √© definidade como \(e^x\)</li>
  <li>O logaritmo de \(e\) √© definido como: \(\log_e x \rightarrow ln(x)\)</li>
</ul>

<h1 id="parte-2---limites">Parte 2 - Limites</h1>
<p>Vamos come√ßar definindo formalmente o que √© limite:</p>

<p><strong>Defini√ß√£o 1:</strong> seja \(f\) uma fun√ß√£o definida sobre algum intervalo aberto que cont√©m o n√∫mero \(b\), exceto possivelmente no pr√≥prio \(b\), ent√£o dizemos que o <strong>limite de</strong> \(f(x)\) <strong>quando</strong> \(x\) <strong>tende a</strong> \(b\) <strong>√©</strong> \(L\) se para todo n√∫mero \(\epsilon &gt; 0\) h√° um n√∫mero correspondente \(\delta &gt; 0\) tal que:</p>

\[\mid f(x) - L \mid &lt; \epsilon \textrm{ sempre que } 0 &lt; \mid x - b \mid &lt; \delta\]

<p>A representa√ß√£o matem√°tica do limite √© descrita a seguir:</p>

\[\lim_{x \to b} f(x) = L
\tag{5}\]

<p>Agora largando o formalismo de lado, o que essa defini√ß√£o nos diz √© o seguinte:</p>
<ul>
  <li>O \(\lim_{x \to b} f(x) = L\) se pudermos tornar os valores de \(f(x)\) arbitrariamente pr√≥ximos de \(L\) tornando \(x\) pr√≥ximo de \(b\), mas n√£o igual a \(b\).</li>
</ul>

<p><strong>Defini√ß√£o 2:</strong> O \(\lim_{x \to b} f(x) = L\) se e somente se \(\lim_{x \to b^+} f(x) = L\) e \(\lim_{x \to b^-} f(x) = L\).</p>

<p>A defini√ß√£o 2 nos diz que aproximando \(x\) a \(b\) pela esquerda ou pela direita, o limite tem que ser o mesmo.</p>

<h2 id="limites-infinitos">Limites infinitos</h2>
<p>Pode acontecer que a medida que \(x\) se aproxima de \(b\) os valores de \(f(x)\) torna-se muito grandes. Vamos tomar como exemplo a fun√ß√£o logaritmica definida anteriormente e plotada no gr√°fico da Figura 2. Portanto, tomando \(f(x) = \log_2 x\) queremos calcular o \(\lim_{x \to 0} \log_2 x\). Se observarmos a Figura 2, quanto mais o valor de \(x\) se aproxima de \(0\), o valor de \(f(x)\) se torna muito grande, mas negativo. Agora lembre-se que o logaritmo n√£o √© definido em \(0\). Nesse caso, dizemos que:</p>

\[\lim_{x \to 0^+} \log_2 x = - \infty\]

<p>Em outras palavras, quando o valor \(x\) se aproxima de \(0\) pela direita, mas n√£o igualando a \(0\), o valor de \(f(x)\) tende a um n√∫mero muito grande negativo, no qual chamamos de \(- \infty\). O mesmo ocorre se aproximarmos \(x\) a \(0\) pela esquerda, mas neste caso o limite tende a \(+ \infty\).</p>

<p>Com isso, √© gerado uma <strong>ass√≠ntota</strong>, que j√° discutimos na parte 1 e √© ilustrada na Figura 2. A curva \(f(x)\) se aproxima o quanto imaginarmos, mas jamais toca o eixo \(y\), uma vez que a fun√ß√£o n√£o √© definida em \(0\).</p>

<h2 id="propriedades">Propriedades</h2>
<p>Existem dezenas de propriedades para se calcular um limite. Como esta aula √© apenas uma revis√£o, vamos descrever apenas algumas delas. Para isso, considere \(c\) uma constante e suponha que o \(\lim_{x \to a} f(x)\) e \(\lim_{x \to a} f(x)\) existam. Sendo assim:</p>

<ul>
  <li>
    <p>P1: \(\lim_{x \to a} [f(x) \pm  g(x)] = \lim_{x \to a} f(x) \pm  \lim_{x \to a} g(x)\)</p>
  </li>
  <li>
    <p>P2: \(\lim_{x \to a} [cf(x)] = c \lim_{x \to a} f(x)\)</p>
  </li>
  <li>
    <p>P3: \(\lim_{x \to a} [f(x) \times g(x)] = \lim_{x \to a} f(x) \times \lim_{x \to a} g(x)\)</p>
  </li>
  <li>
    <p>P4: \(\lim_{x \to a} [f(x) \div g(x)] = \lim_{x \to a} f(x) \div \lim_{x \to a} g(x)\) se \(\lim_{x \to a} g(x) \neq 0\)</p>
  </li>
  <li>
    <p>P5: \(\lim_{x \to a} [f(x)]^n = [\lim_{x \to a} f(x)]^n\)</p>
  </li>
  <li>
    <p>P6: \(\lim_{x \to a} \sqrt[n]{f(x)} = sqrt[n]{\lim_{x \to a} f(x)}\)</p>
  </li>
  <li>
    <p>P7: \(\lim_{x \to a} c = c\)</p>
  </li>
  <li>
    <p>P8: \(\lim_{x \to a} x^n = a^n\)</p>
  </li>
</ul>

<p>Portanto, essas defini√ß√µes s√£o extremamente √∫teis para se calcular o limite de uma fun√ß√£o.</p>

<h2 id="regra-da-substitui√ß√£o-direta">Regra da substitui√ß√£o direta</h2>
<p>Um das regras mais √∫teis para se calcular limite √© o da <strong>substitui√ß√£o direta</strong>. Juntamente com as propriedades anteriores, somos capazes de calcular o limite de uma grande gama de fun√ß√µes. Mas antes de definir a regra, precisamos definir <strong>continuidade</strong>.</p>

<p><strong>Defini√ß√£o 3:</strong> Uma fun√ß√£o \(f(x)\) √© cont√≠nua em \(a\) se:</p>

\[\lim_{x \to a} f(x) = f(a)\]

<p>Da mesma forma, \(f(x)\) √© <strong>cont√≠nua a direita</strong> se  \(\lim_{x \to a^+} f(x) = f(a)\) e a <strong>esquerda</strong> se \(\lim_{x \to a^-} f(x) = f(a)\). Por fim, \(f(x)\) √© <strong>cont√≠nua em um intervalo</strong> se a fun√ß√£o √© cont√≠nua para <strong>todos</strong> os pontos deste intervalo.</p>

<p>Para complementar a defini√ß√£o 3, existe o seguinte teorema:</p>

<p><strong>Teorema 1:</strong> os seguintes tipos de fun√ß√£o s√£o cont√≠nuas em <strong>todo n√∫mero de seus dom√≠nios:</strong></p>

<ul>
  <li>Polin√¥mio</li>
  <li>Fun√ß√µes racionais</li>
  <li>Fun√ß√µes ra√≠zes</li>
  <li>Fun√ß√µes exponenciais</li>
  <li>Fun√ß√µes trigonom√©tricas</li>
  <li>Fun√ß√µes logaritmimicas</li>
  <li>Fun√ß√µes trigonom√©tricas inversas</li>
</ul>

<p>Agora vamos a defini√ß√£o da regra da substitui√ß√£o direta, que √© bem simples:</p>

<p>Portanto, a partir da defini√ß√£o e do teorema acima, temo as <strong>regra da subsitui√ß√£o direta</strong>, ou seja, caso \(f(x)\) seja cont√≠nua em \(a\), para calcular o limite de \(x \to a\) basta calcular \(f(a)\).</p>

<h2 id="c√°lculo-da-reta-tangente">C√°lculo da reta tangente</h2>
<p>Dado o gr√°fico obtido pela equa√ß√£o \(y = f(x)\), se quisermos encontrar a reta tangente a curva em um ponto \(P(x_0, f(x_0))\), fazemos o seguinte:</p>
<ul>
  <li>Definir a reta tangente \(t\) de inclina√ß√£o \(m\) que passa por \(P\)</li>
  <li>Considerar um ponto vizinho \(Q(x, f(x))\), com \(x \neq a\)</li>
  <li>Calcular a inclina√ß√£o da curva \(PQ\) da seguinte forma (da √°lgebra linear do ensino m√©dio):</li>
</ul>

\[m_{PQ} = \frac{f(x) - f(a)}{x-a}
\tag{6}\]

<p>Ao tra√ßar a reta \(PQ\) obtemos uma reta secante a curva \(y\). A ideia √© mover essa curva at√© ela se tornar uma tangente no ponto \(P\). A Figura 3 ilustra essa opera√ß√£o.</p>

<figure style="width: 490px; height: 450px;" class="align-center">
  
  <img src="http://0.0.0.0:4000/assets/img/cursoML/aula1-2/tang.png" alt="" />

  <figcaption style="text-align: center;">
    Figura 3: Estrat√©gia para c√°lculo da reta tangente a partir da reta secante. <a href="http://engenhariaexercicios.com.br" target="blank"> Fonte </a>
  </figcaption>

</figure>

<ul>
  <li>Para executar essa estrat√©gia precisamos fazer \(Q\) se aproximar de \(P\). Para isso fazemos \(x \to x_0\).</li>
  <li>Se \(m_{PQ} \to m\), ent√£o a tangente \(t\) ser√° a reta que passa em \(P\) com inclina√ß√£o \(m\).</li>
  <li>Isso implica que a reta tangente √© a posi√ß√£o <strong>limite</strong> da reta secante, como ilustrado no gr√°fico.</li>
</ul>

<p>De acordo com os passos acima, podemos definir a reta tangente da seguinte forma:</p>

<p><strong>Defini√ß√£o 4:</strong> A reta tangente a uma curva \(y = f(x)\) em um ponto \(P(a, f(a))\) √© a reta que passa por \(P\) com inclina√ß√£o:</p>

\[m = \lim_{x \to a} \frac{f(x)-f(a)}{x-a}
\tag{7}\]

<p>Desde que o limite exista. Tamb√©m √© comum encontrar a equa√ß√£o 7 da seguinte forma:</p>

\[m = \lim_{h \to 0} \frac{f(a+h)-f(a)}{h} \rightarrow h = x - a
\tag{8}\]

<p>Neste momento voc√™ pode est√° se perguntando: mas pra que estou lendo sobre c√°lculo da reta tangente em um curso de <em>machine learning</em>? Bom, esse √© o passo inicial para a compreens√£o de derivadas, o que √© fundamental no universo de ML. Na verdade, a equa√ß√£o 8 √© t√£o comum na ci√™ncia e engenharia que ela ganhou um nome especial: <strong>derivada</strong>. A inclina√ß√£o da reta tangente no ponto \(P\) √© a <strong>interpreta√ß√£o geom√©trica</strong> do que √© derivada. Isso abre caminho para √∫ltima parte desta aula.</p>

<h1 id="parte-3---derivadas">Parte 3 - Derivadas</h1>
<p>A defini√ß√£o de derivada √© basicamente a mesma da defina√ß√£o 4. Mas de qualquer forma:</p>

<p><strong>Defini√ß√£o 5:</strong> a derivada de uma fun√ß√£o \(f\) em um n√∫mero \(a\), denotada por \(f'(a)\) ou \(\frac{d(fa)}{da}\) √© definida como:</p>

\[f'(a) = \lim_{h \to 0} \frac{f(a+h)-f(a)}{h} \rightarrow h = x - a
\tag{9}\]

<p>se o limite existir.</p>

<p>A interpreta√ß√£o geom√©trica da derivada n√≥s j√° sabemos, ou seja, a inclina√ß√£o da reta tangente no ponto \((a, f(a))\). Por√©m, a interpreta√ß√£o mais importante √© como <strong>taxa de varia√ß√£o</strong>.</p>

<h2 id="derivada-como-taxa-de-varia√ß√£o">Derivada como taxa de varia√ß√£o</h2>
<p>A derivada pode ser interpretada como a <strong>taxa de varia√ß√£o inst√¢ntanea</strong> de \(y=f(x)\) em rela√ß√£o a \(x\) quando \(x = a\).</p>

<p>Vamos definir a taxa de varia√ß√£o inst√¢ntanea de \(y = f(x)\) em rela√ß√£o a \(x\) em \(x=x_1\). Se o intervalor for \([x_1, x_2]\), ent√£o a varia√ß√£o \(\Delta x = x_2 - x_1\) √© correspondente em \(y\) como \(\Delta y = f(x_2)-f(x_1)\). Com isso, a <strong>taxa de varia√ß√£o inst√¢ntanea</strong> √© c√°lculada da seguinte forma:</p>

\[t = \lim_{\Delta x \to 0} \frac{\Delta y}{\Delta x} = \lim_{x_1 \to x_2} \frac{f(x_2)-f(x_1)}{x_2 - x_1}
\tag{10}\]

<p>Com isso, a derivada \(\frac{d f(a)}{da}\) √© a taxa de varia√ß√£o inst√¢ntanea de \(y = f(x)\) em rela√ß√£o a \(x\) quando \(x = a\).</p>

<h2 id="fun√ß√µes-diferenci√°veis">Fun√ß√µes diferenci√°veis</h2>
<p>Nem toda fun√ß√£o √© diferenci√°vel e isso √© importante em <em>machine learning</em>. Por isso temos o seguinte teorema:</p>

<p><strong>Teorema 2:</strong> se \(f(x)\) √© diferenci√°vel em \(x = a\), ent√£o \(f\) √© cont√≠nua neste ponto.</p>

<p>Este teorema est√° de acordo com a defini√ß√£o 3. Por√©m, aten√ß√£o! A <strong>rec√≠proca n√£o √© verdadeira</strong>! Por exemplo, a fun√ß√£o \(f(x) = \mid x \mid\) √© cont√≠nua, por√©m ela n√£o √© diferenci√°vel em \(0\), uma vez que n√£o √© poss√≠vel calcular a reta tangente a este ponto.</p>

<p>De maneira geral, voc√™ pode considerar que se uma curva \(y = f(x)\) muda abruptamente de dire√ß√£o, formando uma quina, uma dobra ou uma tangente vertical, a fun√ß√£o <strong>n√£o √© diferenci√°vel no ponto pois n√£o ter√° tangente</strong>.</p>

<h2 id="regras-de-diferencia√ß√£o">Regras de diferencia√ß√£o</h2>
<p>Nesta subse√ß√£o vamos apresentar as principais regras de diferencia√ß√£o at√© a regra da cadeia, que √© de extrema import√¢ncia para <em>machine learning</em>. Lembrando que isso √© apenas uma revis√£o, para provas e vers√µes completas, recorra a refer√™ncia disponibilizada. Para facilitar a escrita, considerando a \(\frac{dq(x)}{dx}\) como \(q'(x)\), sendo \(q(x)\) uma fun√ß√£o diferenci√°vel, temos as seguintes regras:</p>

<ul>
  <li><strong>Constante</strong>: \(f'(c) = 0\)</li>
  <li><strong>Multiplica√ß√£o por constante</strong>: \([c f(x)]' = cf'(x)\)</li>
  <li><strong>Fun√ß√£o pot√™ncia</strong>: \(f'(x^n) = nx^{n-1}\)</li>
  <li><strong>Regra da soma/diferen√ßa</strong>: \([f(x) \pm g(x)]' = f'(x) \pm g'(x)\)</li>
  <li><strong>Regra do produto</strong>: \([f(x) g(x)]' = f'(x)g(x) + f(x)g'(x)\)</li>
  <li><strong>Regra do quociente</strong>: \([\frac{f(x)}{g(x)}]' = \frac{f'(x)g(x) - f(x)g'(x)}{g(x)^2}\)</li>
</ul>

<p>Obviamente podemos compor essas regras para obtermos as derivadas desejadas. Mais pra frente teremos alguns exemplos.</p>

<h3 id="derivadas-de-algumas-fun√ß√µes-especiais">Derivadas de algumas fun√ß√µes especiais</h3>
<p>Algumas fun√ß√µes s√£o bem frequentes, como as trigonom√©tricas e logaritmicas. As derivadas dessas fun√ß√µes s√£o definidas como:</p>

<ul>
  <li><strong>Derivada de \(e^x\)</strong>: \([e^x]' = e^x\)</li>
  <li><strong>Derivadas trigonom√©tricas</strong>:
    <ul>
      <li>Seno: \([sen(x)]' = cos(x)\)</li>
      <li>Cosseno: \([cos(x)]' = -sen(x)\)</li>
      <li>Tangente: \([tg(x)]' = sen^2(x)\)</li>
    </ul>
  </li>
  <li><strong>Derivada de fun√ß√µes logaritmica</strong>
    <ul>
      <li>Base \(a\): \([\log_a x]' = \frac{1}{x \ln a}\)</li>
      <li>Base \(e\): \([\log x]' = \frac{1}{x}\)</li>
    </ul>

    <p>O Interessante das derivadas logaritmicas √© que ela pode ser usada para facilitar derivadas complicadas. Obviamente isso √© utilizado em <em>machine learning</em>. Por exemplo, considere \(y = \frac{x^{3/4} \sqrt{x^2 + 1}}{(3x + 2)^5}\). A primeira vista, calcular essa derivada parece muito dif√≠cil! Mas vamos aplicar \(\ln\) em ambos os lados da equa√ß√£o e em seguida aplicar as regras anteriormente definidas:</p>

\[\ln y = \ln \left ( \frac{x^{3/4} \sqrt{x^2 + 1}}{(3x + 2)^5} \right ) \\
\ln y = \ln(x^{3/4}) + \ln(\sqrt{x^2+1})-5(\ln 3x + \ln 2) \because \textrm{ Regras de log} \\
\ln y = 3/4 \ln x + 1/2 \ln(x^2 + 1) - 5(\ln 3x + \ln 2) \because \textrm{ Regras de log} \\
\frac{1}{y}y' = \frac{3}{4} \frac{1}{x} \frac{1}{2} \frac{1}{x¬≤ + 1} 2x - 5 \frac{1}{3x} 3 + \frac{1}{2} \because \textrm{ Regras de dif.} \\
y' = y \left( \frac{3}{4x} + \frac{x}{x^2 + 1} - \frac{15}{3x + 2} \right ) \because \textrm{ Substituir y} \\\]
  </li>
</ul>

<h3 id="regra-da-cadeia">Regra da cadeia</h3>
<p>A regra da cadeia √©, talvez, a mais importante regra de diferencia√ß√£o e √© parte fundamental do <em>backpropagation</em>, que voc√™ vai ouvir falar muito ainda. Logo, vamos defin√≠-la:</p>

<p><strong>Defini√ß√£o 5:</strong> Seja \(f(x)\) e \(g(x)\) duas fun√ß√µes diferenci√°veis e seja \(F = f(g(x))\), ent√£o \(F'\) √© definida como:
\(F'(x) = f'(g(x))g'(x)
\tag{12}\)</p>

<p>A regra tamb√©m √© encontrada na seguinte nota√ß√£o: seja \(y = f(u)\) e \(u = g(x)\), ent√£o:</p>

\[\frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}
\tag{13}\]

<ul>
  <li><strong>Exemplo 1:</strong> Calcular deriva de \(F(x) = \sqrt{x^2 + 1}\).
    <ol>
      <li>Primeiramente, manipulando a fun√ß√£o:  \(F(x) = {x^2 + 1}^{1/2}\)</li>
      <li>Aplicando a regra da cadeia e as regras de diferencia√ß√£o:
\(F'(x) = 1/2(x^2 + 1)^{-1/2}(2x) = x(x^2 + 1)^{-1/2}\)</li>
    </ol>
  </li>
  <li><strong>Exemplo 2:</strong> Calcular deriva de \(F(x) = cos(x^2)\).
    <ol>
      <li>Aplicando a regra da cadeia:
\(F'(x) = 2x (cos(x^2))\)</li>
    </ol>
  </li>
</ul>

<h2 id="derivadas-superiores">Derivadas superiores</h2>
<p>At√© o momento, foi aprensentado derivadas de primeira ordem. Por√©m, podemos ter derivadas de 1¬™, 2¬™, 3¬™ etc ordens. Portanto, se uma fun√ß√£o \(f(x)\) √© diferenci√°vel, \(f'(x)\) tamb√©m √© uma fun√ß√£o, logo, pode ter sua pr√≥pria derivadas \((f'(x))'\). Essa derivada √© chamada de <strong>derivada segunda</strong>:</p>

\[\frac{d}{dx} \frac{dy}{dx} = \frac{d^2 y}{dx^2}
\tag{14}\]

<h2 id="derivadas-parciais">Derivadas parciais</h2>
<p>At√© o momento tratamos de fun√ß√£o de uma vari√°vel, no caso \(f(x)\). Por√©m, podemos ter fun√ß√µes de mais de uma vari√°vel, por exemplo, \(f(x,y)\). √â muito comum desejarmos calcular a derivada desse tipo de fun√ß√£o. Quando isso ocorre, chamamos essa derivada de <strong>derivada parcial</strong>, e voc√™ vai entender o motivo em instantes.</p>

<p>Primeiramente, √© importante pontuar que a derivada parcial seguem <strong>os mesmos princ√≠pios</strong> da derivada de uma vari√°vel. Portanto, vamos seguir as mesmas regras, por√©m, com duas vari√°veis. A interpreta√ß√£o tamb√©m √© id√™ntica aos postulados anteriores, obviamente, levando em considera√ß√£o as duas vari√°veis.</p>

<p>O importante aqui √© entender que, para duas vari√°veis, teremos duas derivadas, uma com respeito a \(x\) e outra a \(y\). Para mais vari√°veis √© a mesma ideia. Por isso a ideia de parcial, n√£o temos um √∫nico valor, mas parcelas. A nota√ß√£o utilizada √©:</p>

<ul>
  <li>Derivada parcial de \(f(x,y)\) com respeito a \(x\) : \(\frac{\partial f(x,y)}{\partial x}\)</li>
  <li>Derivada parcial de \(f(x,y)\) com respeito a \(y\) : \(\frac{\partial f(x,y)}{\partial y}\)</li>
</ul>

<p>A regra geral para deterinar as derivadas parciais de uma fun√ß√£o \(z = f(x,y)\) √© muito simples e √© definida 2 passos:</p>
<ol>
  <li>
    <p>Para calcular \(\frac{\partial f(x,y)}{\partial x}\), olhe para \(y\) como uma constante e derive a fun√ß√£o \(f(x,y)\) normalmente com respeito a \(x\)</p>
  </li>
  <li>
    <p>Para calcular \(\frac{\partial f(x,y)}{\partial y}\), olhe para \(x\) como uma constante e derive a fun√ß√£o \(f(x,y)\) normalmente com respeito a \(y\)</p>
  </li>
</ol>

<ul>
  <li><strong>Exemplo:</strong> calcular as derivadas parciais de \(f(x,y) = 2x + y^2\)
    <ol>
      <li>Com respeito a \(x\) : \(\frac{\partial f(x,y)}{\partial x} = 2\)</li>
      <li>Com respeito a \(y\) : \(\frac{\partial f(x,y)}{\partial y} = 2y\)</li>
    </ol>
  </li>
</ul>

<p><a name="parte4"></a></p>

<h1 id="parte-4---maximiza√ß√£ominimiza√ß√£o-via-derivadas">Parte 4 - Maximiza√ß√£o/Minimiza√ß√£o via derivadas</h1>
<p>Essa parte √© a mais importante desta aula pois ser√° extremamente utilizada nos algoritmos de minimiza√ß√£o de redes neurais. Para iniciar o tema, vamos definir o <strong>teorema de Fermat</strong> que √© fundamental para que a m√°gica aconte√ßa:</p>

<p><strong>Teorema 3</strong>: Se uma fun√ß√£o \(f(x)\) tiver um m√°ximo ou um m√≠nimo local em \(c\) e \(f'(c)\) existir, ent√£o \(f'(c) = 0\).</p>

<p>Em outras palavras, o teorema diz que ao igualar a derivada de uma fun√ß√£o a zero, estamos encontrando os m√≠nimos e m√°ximos dessa fun√ß√£o. <strong>Por√©m, o inverso do teorema n√£o se aplica.</strong> Por exemplo, lembra da fun√ß√£o \(\mid x \mid\), ela possui m√≠nimo em \(0\), por√©m, como j√° discutimos, a derivada n√£o existe neste ponto.</p>

<p>Sendo assim, para encontrar os valores de m√°ximo e m√≠nimo absolutos de uma fun√ß√£o cont√≠nua \(f(x)\) em um intervalo \([a,b]\), basta seguir os seguintes passos:</p>

<ol>
  <li>Determinar o(s) ponto(s) na(s) qual(is) a \(f'(x) = 0\) em \([a,b]\).</li>
  <li>Calcular os valores de \(f(x)\) nos extremos do intervalo</li>
  <li>O maior valor calculado nos passos anteriores √© o m√°ximo absoluto e o menor o m√≠nimo absoluto.</li>
</ol>

<ul>
  <li><strong>Exemplo</strong>: calcular os valores do m√°ximo e m√≠nimos global de \(f(x) = x^3 - 3x^2 + 1\) no intervalo \([\frac{-1}{2}, 4]\).
    <ol>
      <li>Derivando: \(f'(x) = 3x^2 - 6x = \underline{3x(x - 2)}\)
        <ul>
          <li>Fazendo \(f'(x) =\), encontramos os pontos \(x= 0\) e \(x = 2\).</li>
        </ul>
      </li>
      <li>Calculando os valores extremos: \(f(-1/2) = 1/8\) e \(f(4) = 7\)</li>
      <li>O ponto de m√°ximo √© \((4,17)\) e o de m√≠nimo \((2,-3)\).</li>
    </ol>
  </li>
</ul>

<p>Se observarmos o gr√°fico da fun√ß√£o na Figura 4, podemos ver o m√°ximo e o m√≠nimo. Al√©m disso, caso a derivada exista, a reta tangente ao m√°ximo ou ao m√≠nimo √© <strong>horizontal</strong>. Podemos observar isso no ponto \((2,-3)\).</p>

<figure style="width: 400px; height: 430px;" class="align-center">
  
  <img src="http://0.0.0.0:4000/assets/img/cursoML/aula1-2/min-max.png" alt="" />

  <figcaption style="text-align: center;">
    Figura 4: Gr√°fico da que mostra o m√≠nimo (azul) e m√°ximo (vermelho) global da fun√ß√£o no intervalo desejado<a href="https://www.amazon.com.br/C%C3%A1lculo-1-James-Stewart/dp/8522112584" target="blank"> Fonte </a>
  </figcaption>

</figure>

<h2 id="derivadas-direcionais-e-vetor-gradiente">Derivadas direcionais e vetor gradiente</h2>
<p>Nesta se√ß√£o vamos falar um pouco da <strong>derivada direcional</strong>, que √© utilizada para determinar a taxa de de varia√ß√£o de uma fun√ß√£o de duas ou mais vari√°veis em qualquer dire√ß√£o.</p>

<p>Da defini√ß√£o de vari√°vel, dado \(z = f(x,y)\), sabemos que as derivadas parcias \(\frac{\partial z}{\partial x}\) e \(\frac{\partial z}{\partial y}\) representam as taxas de varia√ß√£o de \(z\) na dire√ß√£o positiva dos eixos \(x\) e \(y\), que tamb√©m pode ser interpretada como na dire√ß√£o e sentido dos versores \(\vec{i}\) e \(\vec{j}\).</p>

<p>Para iniciar, vamos supor que desejamos calcular a taxa de varia√ß√£o de \(z\) no ponto \((x_0, y_0)\) na dire√ß√£o e sentido de um vetor unit√°rio arbitr√°rio \(\vec{u} = \left \langle a, b  \right \rangle\). Isso √© ilustrado na seguinte figura:</p>

<figure style="width: 400px; height: 430px;" class="align-center">
  
  <img src="http://0.0.0.0:4000/assets/img/cursoML/aula1-2/vecu.png" alt="" />

  <figcaption style="text-align: center;">
    Figura 5: representa√ß√£o do vetor u. <a href="https://www.amazon.com.br/C%C3%A1lculo-1-James-Stewart/dp/8522112584" target="blank"> Fonte </a>
  </figcaption>

</figure>

<p>Agora considere a superf√≠cie 3D \(S\) que √© gerada a partir de \(z = f(x,y)\) e \(z_0 = f(x_0, y_0)\). O ponto \(P(x_0, y_0, z_0)\) pertence a \(S\). O plano vertical que passa por \(P\) na dire√ß√£o de \(\vec{u}\) intercepta \(S\) em uma curva \(C\). <strong>A inclina√ß√£o da reta tangente \(T\) a \(C\) em \(P\) √© a taxa de varia√ß√£o de \(z\) na dire√ß√£o e sentido de \(\vec{u}\)</strong>. Para calcular essa taxa, √© utilizada a mesma estrat√©gia descrita na parte 2 desta aula, por√©m baseado em 2 vari√°veis. Dessa forma, vamos descrever apenas o teorema resultado de todas essa manipula√ß√£o matem√°tica.</p>

<p><strong>Teorema 4:</strong> se \(f(x,y)\) √© uma fun√ß√£o diferenci√°vel em \(x\) e \(y\), ent√£o essa fun√ß√£o possui uma derivada direcional de dire√ß√£o e sentido de qualquer versor de \vec{u} = \left \langle a, b  \right \rangle $$ e:</p>

\[D_u f(x,y) = \frac{\partial z}{\partial x}a + \frac{\partial z}{\partial x}b
\tag{15}\]

<p>Baseado neste teorema, podemos escrever a derivada direcional como produto escalar de dois vetores:</p>

\[D_u f(x,y) = \left \langle \frac{\partial f(x,y)}{\partial x}, \frac{\partial f(x,y)}{\partial x} \right \rangle \left \langle a,b \right \rangle \\
D_u f(x,y) = \left \langle \frac{\partial f(x,y)}{\partial x}, \frac{\partial f(x,y)}{\partial x} \right \rangle \vec{u}
\tag{16}\]

<p>O primeiro vetor √© t√£o importante que recebeu um nome especial: <strong>gradiente</strong>.</p>

<p><strong>Defini√ß√£o 6:</strong> seja \(f(x,y)\) uma fun√ß√£o no \(\mathbb{R}^2\), o gradiente de \(f\) √© a fun√ß√£o vetorial \(\nabla f(x,y)\) definida por:</p>

\[\nabla f(x,y) = \frac{\partial f(x,y)}{\partial x}\vec{i} + \frac{\partial f(x,y)}{\partial x} \vec{j} 
\tag{17}\]

<p>Para fun√ß√µes no \(\mathbb{R}^n\), basta adicionar as vari√°veis no somat√≥rio da equa√ß√£o 17.</p>

<p>Por fim, vamos ao √∫ltimo teorema desta aula, mas que √© de suma import√¢ncia:</p>

<p><strong>Teorema 5:</strong> seja \(f\) uma fun√ß√£o diferenci√°vel no \(\mathbb{R}^n\), o valor m√°ximo da derivada direcional √© \(\mid \nabla f(\vec{x}) \mid\) e ocorre quando \(\vec{u}\) tem a mesma dire√ß√£o e sentido que o vetor gradiente \(\nabla f(\vec{x})\).</p>

<p>Em outras palavras, esse teorema nos diz que a fun√ß√£o \(f\) <strong>aumenta mais depressa na dire√ß√£o e sentido do gradiente \(\nabla f\)</strong>. Isso √© um resultado muito importante! Significa que <strong>se desejarmos maximizar uma determinada fun√ß√£o, o gradiente da mesma aponta para o valor m√°ximo!</strong> E obviamente, se eu inverter a dire√ß√£o deste vetor, ele vai apontar para o m√≠nimo! Isso √© exaustivamente utilizado em <em>machine learning</em> e vamos utilizar esse resultado diretamente muito breve!</p>

<h1 id="considera√ß√µes-finais">Considera√ß√µes finais</h1>
<p>Essa foi mais uma das aulas te√≥ricas que podem at√© ser chatas, mas s√£o importantes para formar a base necess√°ria para √°rea. No fundo, <em>machine learning</em> √© isso, quanto mais voc√™ cava, mais matem√°tica voc√™ encontra. Como bem disse Dijkstra (se voc√™ n√£o sabe que √©, fa√ßa o dever de casa), <em>‚ÄúCi√™ncia da computa√ß√£o tem tanto a ver com o computador como a Astronomia com o telesc√≥pio, a Biologia com o microsc√≥pio, ou a Qu√≠mica com os tubos de ensaio. A Ci√™ncia n√£o estuda ferramentas, mas o que fazemos e o que descobrimos com elas.‚Äù</em> Portanto, a pr√≥xima aula ser√° sobre probabilidade e teoria da informa√ß√£o e assim encerramos todos os conceitos necess√°rios para iniciar na √°rea de <em>machine learning</em>.</p>
:ET