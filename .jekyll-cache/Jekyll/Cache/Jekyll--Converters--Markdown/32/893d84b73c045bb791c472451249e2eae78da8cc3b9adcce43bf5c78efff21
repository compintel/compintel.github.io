I"Eç<h1 id="introdu√ß√£o">Introdu√ß√£o</h1>
<p>Seja bem vindo a aula 1.1 do curso de <em>machine learning</em> do blog. Este √© o primeiro post de aula do curso e a ideia da mesma √© introduzir os conceitos b√°sicos que formar√£o a base necess√°ria para iniciar na √°rea. <em>Machine learning</em> √© fortemente baseado em <strong>√°lgebra linear</strong>, <strong>c√°lculo</strong> e <strong>estat√≠stica</strong>. A aula 1 ser√° divida em 3 posts que abordar√£o cada um deste conceitos b√°sicos. Portanto, come√ßameremos com <strong>√Ålgebra linear</strong>.</p>

<p>Eu sei que muitos tem traumas de √°lgebra (principalmente quem j√° cursou em alguma gradua√ß√£o). Por√©m, um bom entendimento da disciplina √© necess√°rio para aprender <em>machine learning</em>. Portanto, aqui vamos discutir diversos conceitos b√°sicos, mas caso queira se aprofundar, sugiro que procure um livro ou algum curso. Infelizemente, a maioria dos materiais est√£o em ingl√™s (eis a raz√£o de exist√™ncia deste blog!) mas voc√™ encontra alguns <a href="https://www.youtube.com/watch?v=TP2o31y5_GU&amp;list=PLO3hBdfBc4pFef1zn1oZyYXLomL9MiX-C">cursos como este</a> no YouTube.</p>

<h1 id="escalares-vetores-e-matrizes">Escalares, vetores e matrizes</h1>
<p>Os 3 principais tipos de dados da √°lgebra linear s√£o descritos a seguir:</p>
<ul>
  <li>
    <p><strong>Escalares</strong>: um escalar √© apenas um n√∫mero (as vezes conhecido como crisp). Normalmente este n√∫mero pertence a um conjunto, como os n√∫meros reais ou naturais. A nota√ß√£o utilizada para um escalar √© uma letra min√∫scula. Um exemplo de escalar pode ser descrito por \(x \in \mathbb{R}\), ou seja, \(x\) √© um escalar que pertence aos n√∫meros reais, logo, ele pode assumir valores como \(4.2\), \(10.0\), \(8\) etc.</p>
  </li>
  <li>
    <p><strong>Vetores</strong>: um vetor √© uma cole√ß√£o de n√∫meros escalares. Logo, esse objeto √© um conjunto de n√∫meros identificados por um √≠ndice. Um vetor pode conter \(n\) elementos de algum conjunto de n√∫meros, por exemplo, os n√∫meros reais. A nota√ß√£o padr√£o para um vetor √© uma letra m√≠nuscula em negrito, por exemplo \(\mathbf{x}\). No caso, \(x_1\) √© o primeiro elemento deste vetor. Uma outra maneira de representar um vetor √© explicitando seus dados, por exemplo, \(\mathbf{x} = \{x_1, x_2, \cdots  x_n \}\). Neste caso dizemos que \(\mathbf{x} \in \mathbb{R}^n\), ou seja, todos os elementos do vetor s√£o n√∫meros reais.</p>
  </li>
  <li>
    <p><strong>Matrizes</strong>: uma matriz √© uma conjunto de dados de 2 ou mais dimens√µes, na qual cada elemento √© identificado por dois ou mais √≠ndices. A nota√ß√£o padr√£o de uma matriz √© uma letra mai√∫scula em negrito, por exempplo, \(\mathbf{X}\). Tomando como exemplo uma matriz de duas dimens√µes, ela pode conter \(m \times n\) elementos. Neste caso, dizemos que \(\mathbf{X} \in \mathbb{R}^{m \times n}\), ou seja, todos os elementos da matriz s√£o n√∫meros reais com \(m\) linhas e \(n\) colunas. Uma maneira de representar essa matriz √© descrita da seguinte maneira:</p>
  </li>
</ul>

\[\mathbf{X} = 
\begin{bmatrix}
x_{11}&amp; \cdots &amp; x_{1n} \\
\vdots &amp; \ddots &amp; \vdots \\
x_{m1}&amp; \cdots &amp; x_{mn}
\end{bmatrix}
\tag{1}\]

<p>Observe que matrizes √© um conceito que engloba vetores e escalares. Podemos dizer que um vetor √© uma matriz com apenas uma linha ou coluna. Da mesma maneira, podemos dizer que um escalar √© uma matriz ou um vetor com apenas um elemento, por exemplo, \([a_{11}]\)</p>

<h1 id="opera√ß√µes-com-matrizes-vetores-e-escalares">Opera√ß√µes com matrizes, vetores e escalares</h1>
<p>Podemos realizar algumas opera√ß√µes com matrizes. Nesta se√ß√£o vamos abordar transposi√ß√£o, soma, subtra√ß√£o e multiplica√ß√£o por escalar, produto escalar e <em>broadcasting</em>. Todas essas opera√ß√µes s√£o bem simples de serem executadas.</p>

<h2 id="tranposi√ß√£o">Tranposi√ß√£o</h2>
<p>Transpor uma matriz nada mais √© do que trocar suas linhas por suas colunas, ou vice-versa. Tomando como exemplo a equa√ß√£o 1, todos os elementos de \(x_{11}\) at√© \(x_{mn}\) s√£o chamados de <strong>diagonal principal</strong>. A transposi√ß√£o espelha todos os elementos da matriz em torno dessa diagnonal. De maneira formal, a transposi√ß√£o √© descrita por:</p>

\[A^T_{ij} = A_{ji}
\tag{2}\]

<p>Um exemplo pode ser descrito na equa√ß√£o 2 a seguir. Observe que a diagonal principal se mant√©m. Uma regra simples para transposi√ß√£o √© pensar: o que √© linha vira coluna e o que √© coluna vira linha. Sempre funcionar!</p>

\[A = \begin{bmatrix}
1 &amp; 2 &amp; 3\\
4 &amp; 5 &amp; 6 \\
7 &amp; 8 &amp; 9
\end{bmatrix}
\qquad
A^T = \begin{bmatrix}
1 &amp; 4 &amp; 7\\
2 &amp; 5 &amp; 8 \\
3 &amp; 6 &amp; 9
\end{bmatrix}

\tag{2}\]

<p>Vetores tamb√©m podem ser transpostos. Caso ele seja uma linha, ele vira coluna e vice versa, como mostra o exemplo na equa√ß√£o 3. No caso de um escalar, a transposta dele √© ele mesmo, ou seja, \(y^T = y\).</p>

\[\mathbf{x} = \begin{bmatrix}
1\\
2\\
3
\end{bmatrix}
\qquad
\mathbf{x}^T = \begin{bmatrix}
1 &amp; 2 &amp; 3
\end{bmatrix}

\tag{3}\]

<h2 id="soma-e-subtra√ß√£o">Soma e subtra√ß√£o</h2>
<p>Duas matrizes podem ser somadas quando as mesmas possuem a mesma ordem, ou seja, o mesmo n√∫mero de linhas e colunas. A soma √© realizada simplemente somando cada elemento de cada posi√ß√£o, como descrito na equa√ß√£o 4 e exemplificado na 5. Os vetores funcionam da mesma maneira, com a diferen√ßa que estaremos somando uma linha ou coluna apenas.</p>

\[C = A + B \rightarrow C_{ij} = A_{ij} + B_{ij}

\tag{4}\]

\[\begin{bmatrix}
1 &amp; 2\\
3 &amp; 4
\end{bmatrix}
+
\begin{bmatrix}
5 &amp; 6 \\
7 &amp; 8
\end{bmatrix}
=
\begin{bmatrix}
6 &amp; 8 \\
10 &amp; 12
\end{bmatrix}

\tag{5}\]

<p>A subtra√ß√£o funciona da mesma maneira basta apenas trocar o sinal.</p>

<h2 id="multiplica√ß√£o-por-escalares">Multiplica√ß√£o por escalares</h2>
<p>Tamb√©m podemos multiplicar matrizes e vetores por escalares. A opera√ß√£o √© bem simples e √© descrita pela equa√ß√£o 6 e exemplificada pela 7. Em resumo, basta realizar a multiplica√ß√£o do escalar por todos os elementos do vetor ou matriz.</p>

\[C = A \times a
\tag{6}\]

\[\begin{bmatrix}
1 &amp; 2\\
3 &amp; 4
\end{bmatrix}
\times
2
=
\begin{bmatrix}
2 &amp; 4\\
6 &amp; 8
\end{bmatrix}
\tag{7}\]

<h2 id="produto-escalar-dot-product">Produto escalar (dot product)</h2>
<p>O produto escalar (ou dot product) entre dois vetores de mesma dimens√£o √© definido por:</p>

\[\mathbf{a} \cdot \mathbf{b} = \sum_{i=0}^n a_ib_i = a_1b_1 \cdots a_nb_n\]

<p>Observe que o produto escalar retorna um escalar.</p>

<h2 id="norma-de-um-vetor">Norma de um vetor</h2>
<p>A norma de um vetor √© representada por \(|| \mathbf{x} ||\) √© informalmente conhecida como o ‚Äútamanho‚Äù de um vetor. Existem diversas normas e de maneira geral para uma fun√ß√£o \(f\) ser considerada uma norma, ela precisa satisfazer as seguintes condi√ß√µes:</p>
<ul>
  <li>N√£o negatividade: \(\forall \mathbf{x} \in \mathbb{R}^n \rightarrow f(\mathbf{x}) \geq 0\)</li>
  <li>\(f(\mathbf{x}) = 0\) se e somente se \(\mathbf{x} = 0\)</li>
  <li>Homegeniedade: \(\forall \mathbf{x} \in \mathbb{R}^n, t \in \mathbb{R} \rightarrow f(t \mathbf{x}) =  t f(\mathbf{x})\)</li>
  <li>Inequa√ß√£o triangular: \(\forall \mathbf{x},\mathbf{y} \in \mathbb{R}^n \rightarrow f(\mathbf{x} + \mathbf{y}) \leq f(\mathbf{x}) + f(\mathbf{y})\)</li>
</ul>

<p>Alguns exemplos de normas s√£o:</p>
<ul>
  <li>
    <p>Norma L1: 
\(|| \mathbf{x} || = \sum_{i=1}^n | x_i |\)</p>
  </li>
  <li>
    <p>Norma L2: 
\(|| \mathbf{x} ||_2 = \sqrt{ \sum_{i=1}^n  x_i^2  }\)</p>
  </li>
  <li>
    <p>Norma \(\infty\):
\(|| \mathbf{x} ||_{\infty} = max_i |x_i|\)</p>
  </li>
  <li>
    <p>Norma \(p\):
\(|| \mathbf{x} ||_p = \left ( \sum_{i=1}^n  x_i^p \right )^{\frac{1}{p}}\)</p>
  </li>
</ul>

<p>Perceba que a norma \(p\) √© uma generaliza√ß√£o das demais. Tenha em mente tamb√©m que existem outras diversas normas, basta satisfazer a condi√ß√µes descritas acima.</p>

<p>A norma \(p\) √© muito utilizada em <em>machine learning</em> e de maneira intuitiva, a norma mede a dist√¢ncia de um vetor para com a origem. No caso, a norma L2 √© t√£o utilizada que por vezes √© omitido o n√∫mero 2 e √© denotado apenas \(\parallel  \mathbf{x} \parallel\).</p>

<p>√â muito comum medir o tamanho de um vetor elevando essa norma ao quadrado, que √© <strong>facilmente calculada fazendo</strong> \(\mathbf{x}^T \mathbf{x}\). Isso √© importante para facilitar o c√°lculo de derivadas. Como ainda vamos ver, atualmente, os algoritmos de minimiza√ß√£o utilizados em ML s√£o baseados em derivadas. Sendo assim, a derivada do quadrado da norma L2 com respeito a um elemento de \(\mathbf{x}\) depende apenas do elemento, enquanto da L2 depende de todo o vetor.</p>

<p>Por fim, o produto escalar entre dois vetores pode ser escrito em termos da norma: \(\underline{ \mathbf{x}^T \mathbf{y} = {\parallel  \mathbf{x} \parallel}_2 {\parallel  \mathbf{y} \parallel}_2 \cos \theta }\), where \(\theta\) is the angle between both vectors.</p>

<h2 id="broadcasting">Broadcasting</h2>
<p>Em computa√ß√£o √© comum encontramos a opera√ß√£o de <em>broadcasting</em> (normalmente n√£o existe uma tradu√ß√£o pra essa palavra). Essa opera√ß√£o simplifica a nota√ß√£o e a computa√ß√£o. Python possui um pacote de computa√ß√£o n√∫merica, chamada <a href="https://www.numpy.org/">NumPy</a>, que da suporte a <em>broadcasting</em>. Ela ser√° muito utilizada neste curso.</p>

<p>Explicando o <em>broadcasting</em> por meio de um exemplo, imagine que desejamos somar uma matriz \(A_{m \times n}\) a um vetor \(\mathbf{b}_{m \times 1}\). Bom, sabemos que isso n√£o √© poss√≠vel, uma vez que para a soma ocorrer, ambos devem ter a mesma ordem. Por√©m, podemos repetir os valores a linha do vetor \(n\) vezes e ai sim realizar essa soma. Para evitar escrever tudo isso, apenas dizemos que \(C = A + \mathbf{b}\). Como j√° est√° convencionado o <em>broadcasting</em>, seja \(A = \begin{bmatrix}
1 &amp; 2\\
3 &amp; 4
\end{bmatrix}\) e \(\mathbf{b} = [5, 6]\), sabemos que essa opera√ß√£o significar fazer:</p>

\[\begin{bmatrix}
1 &amp; 2\\
3 &amp; 4
\end{bmatrix}
+
\begin{bmatrix}
5 &amp; 6 \\
5 &amp; 6
\end{bmatrix}
=
\begin{bmatrix}
6 &amp; 8\\
8 &amp; 10
\end{bmatrix}

\tag{8}\]

<h1 id="multiplica√ß√£o-de-matrizes">Multiplica√ß√£o de matrizes</h1>
<p>A multiplica√ß√£o de matrizes √© uma das opera√ß√µes mais importantes envolvendo matrizes. A regra de ouro aqui √© que o n√∫mero de linhas de uma matriz deve ser o mesmo de colunas da matrizes na qual deseja-se multiplicar. Isso √© descrito na equa√ß√£o 9.</p>

\[C = A \times B \rightarrow C_{m \times p} = A_{m \times \underline{n}} \times C_{\underline{n} \times p}

\tag{9}\]

<p>A multiplica√ß√£o de matrizes n√£o como a soma que basta multiplicar todos os elementos de uma matriz pelos os de mesmo √≠ndice da matriz subsequente. Essa opera√ß√£o de multiplicar elemento por elemento √© conhecida como <em>element-wise product</em> e tem um s√≠mbolo especiar, no caso \(A \odot B\). A multiplica√ß√£o de fato √© definida da sequinte maneira:</p>

\[C_{ij} = \sum_k A_{ik} \times B_{kj}
\tag{10}\]

<p>Para exemplificar:</p>

\[\begin{bmatrix}
1 &amp; 2\\
3 &amp; 4
\end{bmatrix}
\times
\begin{bmatrix}
5 &amp; 6 \\
7 &amp; 8
\end{bmatrix}
=
\begin{bmatrix}
1 \times 5 + 2 \times 7 &amp; 1 \times 6 + 2 \times 8 \\
3 \times 5 + 4 \times 7 &amp; 3 \times 6 + 4 \times 8
\end{bmatrix}
=
\begin{bmatrix}
19 &amp; 22 \\
43 &amp; 50
\end{bmatrix}

\tag{11}\]

<p>Se observamos a multiplica√ß√£o acima, percebemos que nada mais √© do que um produto escalar esntre as linhas das matrizes. √â comum representar o produto escalar como sendo \(\mathbf{a}^T \cdot \mathbf{b}\).</p>

<h2 id="propriedades">Propriedades</h2>
<p>A multiplica√ß√£o de matrizes possuem algumas propriedades que podem ser √∫teis em alguns casos. Destacamos as seguintes propriedades:</p>
<ul>
  <li><em>Distributiva</em>: \(A(B+C) = AB + AC\)</li>
  <li><em>Associativa</em>: \(A(BC) = (AB)C\)</li>
</ul>

<p>Por√©m a multiplica√ß√£o n√£o √© comutativa, isto √© \(AB \neq BC\). Por√©m, o producto escalar √©, i.e., \(\mathbf{a}^T\mathbf{b} = \mathbf{b}^T\mathbf{a}\).</p>

<p>Al√©m disso, a transposta de uma matriz possui a seguinte propriedade: \((AB)^T = A^TB^T\).</p>

<p>Existem diversas outras propriedas, mas isso aqui √© apenas uma introdu√ß√£o ent√£o n√£o vamos adentrar nelas e nem nas provas te√≥ricas.</p>

<h2 id="um-simples-sistema-linear">Um simples sistema linear</h2>
<p>Com os conceitos explorados at√© aqui, podemos montar um simples sistema linear da seguinte forma:</p>

\[A \mathbf{x} = \mathbf{b}

\tag{12}\]

<p>Onde \(A \in \mathbb{R}^{m \times n}\), \(\mathbf{b} \in \mathbb{R}^{m}\) e \(\mathbf{x} \in \mathbb{R}^{n}\). Neste caso a vari√°vel \(\mathbf{x}\) √© valor desconhecido que desejamos descobri. Cada elemento \(x_i\) √© um elemento desconhecido a ser descorberto. Utilizando os conhecimentos adquiridos at√© aqui voc√™ pode aferir facilmente que a primeira linha desta equa√ß√£o linear ser√° \(a_{11}x_1 + a_{12}x_2 + \cdots + a_{1_n}x_n = b_1\). Como exerc√≠cio, tente expandir as demais linhas! Essa <strong>modelagem matricial √© de extrema import√¢ncia</strong> para os algoritmos de <em>machine learning</em> uma vez que basicamente tudo √© modelado assim.</p>

<p><strong>Nota:</strong> perceba que n√£o existe divis√£o de matrizes. Voc√™ pode realizar a opera√ß√£o por elemento (<em>element-wise</em>), mas a divis√£o em si n√£o existe.</p>

<h1 id="invers√£o-de-matriz">Invers√£o de matriz</h1>
<p>Para solucionar o sistema linear apresentado na equa√ß√£o 12 vamos precisar uma outra opera√ß√£o matricial chamada de invers√£o. Mas antes, precisamos definir o conceito de matriz identidade. Uma matriz identidade \(I_n \in \mathbb{R}^{n \times n}\) √© definida como a matriz que multiplicada por vetor n√£o o modifica. Formalmente:</p>

\[I_n \mathbf{x} = \mathbf{x} ; \forall \mathbf{x} \in \mathbb{R}^n
\tag{13}\]

<p>Como pode ser observado, uma matriz identidade √© obrigatoriamente quadrada, ou seja, o n√∫mero de linhas √© igual ao n√∫mero de colunas, sua diagnoal principal √© composta por uns e o resante de seus indices por zeros. Um exemplo de matriz identidade de ordem 3 √© mostrado na equa√ß√£o 14:</p>

\[I_3 = 
\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix}

\tag{14}\]

<p>Bom, agora podemos definir a matriz inversa. Essa matriz, denotada por \(A^{-1}\) √© definida como a matriz que multiplicada por \(A\) retorna a identidade. Formalmente:</p>

\[A^{-1}A = I_n

\tag{15}\]

<p>Com essa defini√ß√£o √© poss√≠vel solucionar o sistema linear da equa√ß√£o 12 da seguinte maneira:</p>

\[\begin{split}
A \mathbf{x}  = \mathbf{b} \\
A^{-1} A \mathbf{x}  = A^{-1} \mathbf{b} \\
I_n \mathbf{x}  = A^{-1} \mathbf{b} \\
\underline{\mathbf{x}  = A^{-1} \mathbf{b}}

\end{split}
\tag{16}\]

<p>Observe que foi adicionado \(A^{-1}\) em ambos os lados da equa√ß√£o para que ela fosse solucionada. Para essa equa√ß√£o ser v√°lida a inversa da matriz \(A\) deve ser v√°lida. Acontece que nem sempre a matriz possui uma inversa. Neste caso, quando ela possui uma inversa, dizemos que a matriz √© <strong>invers√≠vel</strong> ou <strong>n√£o-singular</strong>. Caso contr√°rio, dizemos que a matriz √© <strong>n√£o-invers√≠vel</strong> ou <strong>singular</strong>. Vamos discutir isso na pr√≥xima se√ß√£o.</p>

<h2 id="propriedades-1">Propriedades</h2>
<p>Considerando \(A,B \in \mathbb{R}^{n \times n}\) matrizes n√£o singulares, podemos definir as seguintes propriedades para a matriz inversa:</p>
<ul>
  <li>A inversa da inversa \(A\) √© igual a \(A\): \(\underline{(A^{-1})^{-1} = A}\)</li>
  <li>A inversa de \(AB\) √© igual a multiplica√ß√£o de suas respectivas inversas: \(\underline{(AB)^{-1} = B^{-1}A^{-1}}\)</li>
  <li>A transposta da inversa de \(A\) √© igual a inversa da transposta: \(\underline{(A^{-1})^{T} = (A^T)^{-1}}\)</li>
</ul>

<h1 id="depend√™ncia-linear-span-e-posto">Depend√™ncia linear, <em>span</em> e posto</h1>
<p>Para que a inversa de \(A{-1}\) exista √© necess√°rio que o sistema linear da equa√ß√£o 12 tenha <strong>apenas umas solu√ß√£o</strong> para cada valor de \(\mathbf{b}\). Como voc√™ deve lembrar do seu ensino m√©dio (eu espero que lembre), esse tipo de sistema pode ter nenhuma, uma ou v√°rias solu√ß√µes. Neste caso, se \(\mathbf{x}\) e \(\mathbf{y}\) s√£o solu√ß√µes, ent√£o:</p>

\[\mathbf{z} = \alpha \mathbf{x} + (1-\alpha) \mathbf{x}
\tag{17}\]

<p>tamb√©m √© uma solu√ß√£o para qualquer valor de \(\alpha\). Uma maneira de determinar quantas solu√ß√µes a equa√ß√£o possui √© imaginar cada coluna de \(A\) como um vetor e quantificar quantas vezes esse vetor, saindo da origem, gera o vetor \(\mathbf{b}\). Desta maneira, cada elemento de \(\mathbf{x}\) especifica o quando o devemos afastar da origem, sendo \(x_i\) o quanto se afasta em dire√ß√£o a coluna \(i\) da matriz:</p>

\[A \mathbf{z} = \sum_i x_i A_{:,i}
\tag{18}\]

<p>Essa opera√ß√£o nada mais √© do que uma <strong>combina√ß√£o linear</strong>. Formalmente, uma combina√ß√£o linear de um vetor \(\mathbf{u} = \{u_1, u_2, \cdots, u_n \}\) √© dada pela soma da multiplica√ß√£o de cada elemento deste vetor por um escalar:</p>

\[\hat{\mathbf{u}} =  \sum_{i=1}^n \alpha u_i
\tag{19}\]

<p>Dessa forma, o <strong><em>span</em></strong> de um conjunto de vetores √© o conjunto de todos os pontos obtidos atrav√©s de combina√ß√µes lineares dos vetores originais. Sendo assim, imagine um conjunto de vetores \(U =  \{\mathbf{u}_1, \mathbf{u}_2, \cdots, \mathbf{u}_n \}\). Esse conjunto de vetores √© considerado <strong>linearmente independente (LI)</strong> se nenhum ventor pertencente a \(U\) pode ser presentado como combina√ß√£o linear dos demais. Caso isso ocorra, esse conjunto √© considerado <strong>linearmente dependente (LD)</strong>. Caso n√£o tenha entendido, √© simples. Considere a seguinte situa√ß√£o: se \(\mathbf{u}_1\) pode ser obtido atrav√©s uma combina√ß√£o linear, de acordo com a equa√ß√£o 19, dos vetores \(\mathbf{u}_2, \cdots, \mathbf{u}_n\), logo, esse conjunto √© LD, caso contr√°rio, LI.</p>

<p>De maneira formal, para determinar se a equa√ß√£o 12 possui uma solu√ß√£o, precisamos saber se \(\mathbf{b}\) est√° no <strong><em>span</em></strong> das colunas de \(A\). Esse <strong><em>span</em></strong>, em particular, √© conhecido como <strong>espa√ßo das colunas</strong> ou <strong>intervalo</strong> de \(A\). Perceba que encontrar todas as combina√ß√µes lineares de um conjunto de vetores n√£o √© algo trivial. Logo, existem algumas condi√ß√µes que precisam ser respeitadas para que a solu√ß√£o exista.</p>

<p>Tomando \(\mathbf{b} \in \mathbb{R}^m\), √© necess√°rio que o <strong>espa√ßo de colunas</strong> de \(A\) esteja todo em \(\mathbb{R}^m\). Isso implica que \(A\) deve ter pelo menos \(m\) colunas com \(n \ge m\). Todavia, isso √© apenas uma condi√ß√£o necess√°ria, mas n√£o suficiente, pois algumas colunas podem ser redundantes. Essa redund√¢ncia nada mais √© do que um <strong>depend√™ncia linear</strong>. Para verificar essa depend√™ncia introduzimos o conceito de <strong>posto</strong> (ou <em>rank</em>).</p>

<p>Considerando o conjunto de colunas (ou linhas) de uma matriz \(A\), o posto \(A\) √© igual ao maior n√∫mero elementos deste conjunto que s√£o <strong>linearmente independentes</strong>. Esse conjunto pode ser tanto de colunas ou linhas pois ambos os valores de ambos postos s√£o obrigatoriamente iguais. Uma propriedade importante √© que posto(\(A\)) \(\le\) min(\(m,n\)). Se posto(\(A\)) \(=\) min(\(m,n\)), \(A\) √© chamada de <strong>matriz de posto completo</strong> (ou <em>full rank</em>). De maneira did√°tica, se \(A\) √© uma matriz \(5 \times 4\) e o posto(\(A\)) = 5, logo, \(A\) √© posto completo.</p>

<p>Mas por que o posto √© importante? Bom, para garantir que o sistema da equa√ß√£o 12 tenha apenas uma solu√ß√£o, devemos verificar se \(A\) √© posto completo. Isso √© <strong>condi√ß√£o necess√°ria e suficiente</strong> para o que desejamos. Perceba que se juntarmos a condi√ß√£o necess√°ria e a necess√°ria e sufciente apresentadas at√© aqui, isso significa que a matriz \(A\) necess√°riamente deve ser <strong>quadrada</strong>, ou seja, \(m = n\) e com todas as colunas linearmente independentes. Caso uma matriz quadrada possua alguma coluna linearmente dependente, ela √© chamada de <strong>singular</strong>. E se voc√™ se recorda da se√ß√£o anterior, matrizes singulares n√£o s√£o invers√≠ves. Portanto, ap√≥s todos esses conceitos, o resultado final √©: <strong>para uma matriz ser invers√≠vel ela deve ser posto completo!</strong></p>

<h1 id="determinante-de-uma-matriz">Determinante de uma matriz</h1>
<p>O determinante de uma matriz √© uma fun√ß√£o que associa uma <strong>matriz quadrada</strong> a um <strong>escalar</strong>. Este valor pode ser interpretado como uma <strong>medida de singularidade da matriz</strong>. De maneira formal, o determinante de uma matriz quadrada \(A \in \mathbb{R}^{n\times n}\) √© a fun√ß√£o det: \(\mathbb{R}^{n\times n} \rightarrow \mathbb{R}\) e √© denotado por det\(A\) ou \(\mid A \mid\).</p>

<p>A formula geral para calcular o determinate de uma matriz n√£o √© trivial de se entender. Por isso (se voc√™ se lembrar do ensino m√©dio), temos regras especificas para matrizes \(2 \times 2\) e \(3 \times 3\). De qualquer forma, para a matriz \(A\) definida anteriormente, definimos a a submatriz \(\breve{A}\) que ser√° o resultado da exclus√£o da \(i\)-√©sima linha e a \(j\)-√©sima coluna de \(A\). De maneira formal, temos \(\breve{A}_{\mid i, \mid j} \in \mathbb{R}^{(n-1) \times (n-1)}\).</p>

<h2 id="matriz-diagonal">Matriz diagonal</h2>
<p>Uma matriz diagonal √© toda composta por zeros, exceto pela diagonal principal. Formalmente, \(D\) √© uma matriz diagonal se \(D_{ij} == 0\) para todo \(i \ne j\). Um exemplo de matriz diagonal √© a identidade, descrita na equa√ß√£o 14.</p>

<p>Um matriz diagonal quadrada composta por elementos de \(\mathbf{v}\) √© descrita como diag(\(\mathbf{v}\)). Utilizar esse tipo de matriz √© interessante pois a mesma √© computacionalmente eficiente. Para calcular diag(\(\mathbf{v}\)) \(\mathbf{x}\) basta escalar cada elemento de \(x_i\) por \(v_i\). Em outras palavras, diag(\(\mathbf{v}\)) \(\mathbf{x} = \mathbf{v} \times \mathbf{x}\).</p>

<p>Al√©m disso, calcular a inversa de uma matriz diagonal √© bastante eficiente. Caso a diagonal seja n√£o zero, diag\((\mathbf{v})^{-1}\) = diag\(\left ( \left [ \frac{1}{v_1}, \cdots, \frac{1}{v_n}  \right ]^T \right )\). Essa matriz tamb√©m √© conhecida com <strong>matriz de cofatores</strong>. Dessa forma, o determinante de \(A\) √© calculado da seguinte forma:</p>

\[\mid A \mid = 
\begin{matrix}
\sum_{i=1}^{n} (-1)^{i+j} \mid \breve{A}_{\mid i, \mid j} \mid  &amp; \textrm{for any } j \in 1, \cdots, m \\
\sum_{j=1}^{n} (-1)^{i+j} \mid \breve{A}_{\mid i, \mid j} \mid  &amp; \textrm{for any } i \in 1, \cdots, m \\
\end{matrix}
\tag{20}\]

<p>Perceba que a formula √© recursiva e bastante trabalhosa. Por conta disso, √© dif√≠cil encotrar a expans√£o da mesma para matrizes de ordem superior a \(3 \times 3\). A seguir, √© apresentado a expans√£o da equa√ß√£o 20 para \(A_{3 \times 3}\). Caso queira conferir a f√≥rmula, sinta-se a vontade:</p>

\[\begin{matrix}
\left | \begin{bmatrix}
 a_{11}&amp; a_{12} &amp; a_{13} \\ 
a_{21} &amp; a_{22} &amp; a_{23} \\ 
a_{31} &amp; a_{32} &amp; a_{33}
\end{bmatrix} \right | =  &amp; a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - \\
 &amp; - a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33} - a_{13}a_{22}a_{31}

\end{matrix}
\tag{21}\]

<p>O determinante possui algums propriedades interessantes e muito importantes:</p>
<ul>
  <li>Para \(A \in \mathbb{R}^{n \times n}\): \(\mid A \mid = \mid A^T \mid\)</li>
  <li>Para \(A,B \in \mathbb{R}^{n \times n}\): \(\mid AB \mid = \mid A \mid \mid B \mid\)</li>
  <li>Para \(A \in \mathbb{R}^{n \times n}\): \(\mid A \mid = 0\) <strong>se e somente se</strong> \(A\) <strong>√© singular</strong></li>
  <li>Para \(A \in \mathbb{R}^{n \times n}\) e \(\mid A \mid \neq 0\): \(\mid A \mid^{-1} = \frac{1}{\mid A \mid}\)</li>
</ul>

<p>Se voc√™ voltar ao in√≠cio dessa se√ß√£o, definimos que o determinante √© uma medida de singularidade e isso √© descrito na terceira propriedade anterior. Essa propriedade √© muito importante e se voc√™ conectou os pontos ela diz, em outras palavras que: <strong>para uma matriz ser invers√≠vel, obrigat√≥riamente seu determinante deve ser diferente de zero!</strong>. Convenhamos que isso √© muito mais f√°cil do que determinar todoas as combina√ß√µes lineares das colunas da matriz.</p>

<p>Por fim, a partir da defini√ß√£o e das propriedades de determinante, surge a defini√ß√£o da <strong>matriz adjunta</strong>:</p>

\[\textrm{adj}(A) = (-1)^{i+j} \mid A_{\mid i, \mid j} \mid
\tag{22}\]

<p>A partir desta matriz adjunta √© poss√≠vel calcular a inversa de \(A\) da seguinte forma:</p>

\[A^{-1} = \frac{1}{\mid A \mid} \textrm{adj}(A)
\tag{23}\]

<p>Todavia, apesar dessa formula ser expl√≠cita, ela √© computacionalmente custosa de ser computada.</p>

<h1 id="tra√ßo-de-uma-matriz">Tra√ßo de uma matriz</h1>
<p>O tra√ßo (ou <em>trace</em>) de uma matriz quadrada nada mais √© do que a soma da sua diagonal principal. Dessa forma, seja \(A \in \mathbb{R}^{n \times n}\), o seu tra√ßo tr(\(A\)) √© definido como:</p>

\[\textrm(tr)(A) = \sum_{i=1}^n A_{ii}
\tag{24}\]

<p>Algumas das propriedades do tra√ßo de \(A\) s√£o descritas a seguir:</p>
<ul>
  <li>Para \(A \in \mathbb{R}^{n \times n}\): tr(\(A\)) = tr(\(A^T\))</li>
  <li>Para \(A,B \in \mathbb{R}^{n \times n}\): tr(\(A + B\)) = tr(\(A\)) + tr(\(B\))</li>
  <li>Para \(A \in \mathbb{R}^{n \times n}\) e \(t \in \mathbb{R}\): tr(\(tA\)) = \(t\)tr(\(A^T\))</li>
  <li>Para \(A,B \in \mathbb{R}^{n \times n}\) em que \(AB\) √© uma matriz quadrada: tr(\(AB\)) = tr(\(BA\))</li>
</ul>

<p>Vamos utilizar esse conceito mais para frente.</p>

<h1 id="tipos-especiais-de-matrizes-e-vetores">Tipos especiais de matrizes e vetores</h1>
<p>Algumas matrizes e vetores s√£o bastantes √∫teis. Nesta se√ß√£o ser√£o apresentadas algumas:</p>

<h2 id="matriz-sim√©trica">Matriz sim√©trica</h2>
<p>Uma matriz sim√©trica √© definida como $$ A_{ij} = A_{ji}. Ou seja, os valores acima da diagonal √© igual aos abaixo. Isso ocorre, quando calculamos, por exemplo, uma matriz de covari√¢ncia (n√£o se preocupe, chegaremos l√°). Um exemplo de matriz sim√©trica √© descrito a seguir:</p>

\[A = 
\begin{bmatrix}
3 &amp; 5 &amp; 6 \\
5 &amp; 2 &amp; 4 \\
6 &amp; 4 &amp; 8
\end{bmatrix}
\tag{25}\]

<p>Uma propriedade interessante deste tipo de matriz √© que ela √© igual a sua transposta, ou seja,  \(A = A^T\).</p>

<h2 id="vetor-normal-ortogonal-e-ortonormal">Vetor normal, ortogonal e ortonormal</h2>
<p>Um vetor \(\mathbf{x}\) √© considerado normal (ou unit√°rio) se sua norma L2 √© unit√°ria:</p>

\[{\parallel \mathbf{x} \parallel}_2 = 1
\tag{26}\]

<p>Um detalhe importante √© que qualquer vetor com norma maior do que zero pode ser normalizado, basta dividir o mesmo por sua norma, ou seja, se \({\parallel \mathbf{x} \parallel}_2 &gt; 0\) ent√£o \(\frac{\mathbf{x}}{ {\parallel \mathbf{x} \parallel}_2} = 1\).</p>

<p>Dois vetores \(\mathbf{x}\) e \(\mathbf{y}\) s√£o ortogonais entre si se o produto escalar entre eles for igual a zero, ou seja:</p>

\[\mathbf{x}^T \mathbf{y} = 0 
\tag{27}\]

<p>Se a norma de ambos os vetores for diferente de zero, isso significa que o √¢ngulo formado entre os vetores √© igual a 90¬∞. Al√©m disso, se os vetores s√£o ortogonais e ambos possuem normas unit√°rias, eles recebem o nome de <strong>ortonormal</strong></p>

<h2 id="matriz-ortogonal">Matriz ortogonal</h2>
<p>Uma matriz ortogonal √© uma matriz quadrada na qual suas linhas e colunas s√£o mutualmente ortonormais entre si, respectivamente (<strong>importante</strong>: √© linha com linha e coluna com coluna). Caso isso ocorra, a seguinte propriedade √© observada:</p>

\[A^T A = A A^T = I
\tag{28}\]

<p>O que implica em outra <strong>propriedade muito importante</strong>:</p>

\[A ^{-1} = A ^T
\tag{29}\]

<p>Isso significa que a inversa desse tipo de matriz possui um custo computacional <strong>muito baixo</strong> e por isso ela √© interessante. Por fim, uma √∫ltima propriedade importante deste tipo de matriz √©: <strong>o produto entre matrizes ortogonais resulta em outra matriz ortogonal</strong>.</p>

<h1 id="autovalores-e-autovetores">Autovalores e Autovetores</h1>
<p>Muitos conceitos matem√°ticos podem ser melhores compreendidos se formos capazes de quebr√°-los em partes menores ou encontrando propriedades √∫teis. Como por exemplo, um n√∫mero inteiro pode ser decomposto pelos seus fatores. Considere o n√∫mero \(12\). Podemos escrev√™-lo da seguinte forma: \(12 = 2 \times 2 \times 3\). A partir dessa decomposi√ß√£o podemos concluir que \(12\) n√£o √© divis√≠vel por \(5\) e que qualquer m√∫ltiplo dele tamb√©m √© divis√≠vel por \(3\). Desejamos utilizar esse conceito demostrado neste simples exemplo para matrizes. E uma forma de alcan√ßar isso √© utilizando os <strong>autovalores</strong> e <strong>autovetores</strong>. Este conceito √© bem importante e muito utilizados em alguns algoritmos de <em>machine learning</em>. Por conta disso, vamos dar uma aten√ß√£o especial a ele.</p>

<h2 id="defini√ß√£o-e-exemplos">Defini√ß√£o e exemplos</h2>
<p>Vamos come√ßar definindo o que √© um autovalor e autovetor de um operador linear. Seja \(T\) um operador linear tal que \(T: \mathbb{V} \rightarrow \mathbb{V}\), um vetor \(\mathbf{v} \in \mathbb{V}\) tal que \(\mathbf{v} \neq 0\), √© chamado de <strong>autovetor</strong> de \(T\) se existe um n√∫mero real \(\lambda\) tal que:</p>

\[T(\mathbf{v}) = \lambda \mathbf{v}
\tag{30}\]

<p>Neste caso, \(\lambda\) √© denominado <strong>autovalor</strong> de \(T\) associado ao autovetor \(\mathbf{v}\). Pela defini√ß√£o √© poss√≠vel perceber que geometricamente \(\mathbf{v}\) √© um vetor que n√£o muda de dire√ß√£o quando aplicamos o operador \(T\). Em outras palavras, um autovetor √© um vetor que √© levado em um m√∫ltiplo de si pr√≥prio.</p>

<p>Para entendermos melhor, vamos a dois exemplos:</p>

<p><strong>Exemplo 1:</strong>
Considere o operador linear \(T: \mathbb{R}^2 \rightarrow \mathbb{R}^2\) tal que \(T(x,y) = (4x + 5y, 2x + y)\). Agora considere o vetor \(\mathbf{v} = (5,2)\). Aplicando \(T(\mathbf(v))\) temos:</p>

\[T(5,2) = (30,12) = 6 \times (5,2)
\tag{31}\]

<p>Neste caso, \(6\) √© um autovalor associado ao autovetor \((5,2)\) do operador \(T\).</p>

<p><strong>Exemplo 2:</strong>
Considere o operador linear \(T: \mathbb{R}^3 \rightarrow \mathbb{R}^3\) tal que \(T(x,y, z) = (x, y, 0)\). Neste caso, observe que a seguinte equa√ß√£o √© valida:</p>

\[T(x, y, 0) = 1 \times (x, y, 0)
\tag{32}\]

<p>Neste caso, qualquer vetor \((x, y, 0)\) √© um autovetor de \(T\) com autovalor igual a \(1\). Este exemplo mostra que um mesmo autovalor pode ser associando a v√°rios autovetores.</p>

<h2 id="determinando-autovalores-e-autovetores">Determinando autovalores e autovetores</h2>
<p>Vamos entender o conceito atrav√©s de um exemplo simples. Considere o operador linear \(T: \mathbb{R}^2 \rightarrow \mathbb{R}^2\) descrito por \(T(x,y) = (ax + by, cx + dy)\). Determinar os autovalores e autovetores √© a mesma coisa que: encontrar \(\lambda \in \mathbb{R}\) tal que exista \((x, y) \neq (0, 0)\) e \(T(x,y) = \lambda \times (x,y)\) .</p>

<p>Isso √© o mesmo que encontrar:</p>

\[\left\{\begin{matrix}
ax + by = \lambda x \\
cx + dy = \lambda y
\end{matrix}\right.
\Rightarrow 
\left\{\begin{matrix}
(a-\lambda)x + by = 0 \\
cx + (d-\lambda)y = 0
\end{matrix}\right.
\tag{33}\]

<p>na qual \((x, y) \neq (0, 0)\). A t√≠tulo de curiosidade, o polin√¥mio \(p(\lambda)\) obtido por esta equa√ß√£o √© chamado de polin√¥mio caracter√≠stico.</p>

<p>Coincidentemente (s√≥ que n√£o), ca√≠mos em um sistema linear igual ao da equa√ß√£o 12. Se ainda est√° por dentro dessa aula, voc√™ vai se lembrar que esse sistema possui uma solu√ß√£o n√£o-nula se e somente se:</p>

\[\textrm{det}\begin{bmatrix}
(a-\lambda) &amp; b \\
c &amp; (d-\lambda)
\end{bmatrix}
= 0 
\tag{34}\]

<p>Portanto, o <strong>autovalores</strong> de \(T\) s√£o encontrados atrav√©s da equa√ß√£o 34, se existirem. Por sua vez, os <strong>autovetores</strong> associados a cada autovalor \(\lambda\) s√£o as solu√ß√µes n√£o nulas da equa√ß√£o 33.</p>

<h2 id="decomposi√ß√£o-matricial-por-autovalores-e-autovetores">Decomposi√ß√£o matricial por autovalores e autovetores</h2>
<p>Para realizar uma decomposi√ß√£o matricial utilizando autovalores e autovetores temos que calcul√°-los para uma dada matriz. Todavia, j√° fizemos isso nas subse√ß√µes anteriores e s√≥ vamos ‚Äúsubstituir‚Äù o operador linear \(T\) pela matriz. Portanto, dado uma matriz \(A \in \mathbb{R}^n\), os autovalores e autovetores s√£o obtidos da seguinte forma:</p>

\[A \mathbf{v} = \lambda \mathbf{v}
\tag{35}\]

<p>Se voc√™ observar as equa√ß√µes 33 e 34 vai perceber que estamos fazendo exatamente a mesma coisa mas com uma nota√ß√£o diferente. Logo, para determinar seus autovalores e autovetores, vamos reescrever a equa√ß√£o 34 da seguinte forma:</p>

\[\textrm{det}(A - \lambda I_n) = 0
\tag{36}\]

<p>Portanto, \(\mathbf{v}\) √© um autovetor de \(A\) associado a um autovalor \(\lambda\) se e somente se ambos satisfazem a equa√ß√£o 36. Caso queira um exemplo num√©rico, <a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Matrix_examples">neste artigo da Wikipedia</a> voc√™ encontra um excelente exemplo inclusive com uma ilustra√ß√£o bem legal do significado geom√©trico.</p>

<p>Dado a defini√ß√£o, agora temos que adentrar para a decomposi√ß√£o de fato. Por√©m, √© necess√°rio um teorema, que √© descrito a seguir:</p>
<ul>
  <li>Se \(\mathbf{v}_1, \cdots, \mathbf{v}_n\) s√£o autovetores de uma matriz \(A\) com <strong>diferentes</strong> autovalores \(\lambda_1, \cdots, \lambda_n\) associados, ent√£o o conjunto de autovetores \(\mathbf{v}\) √© <strong>linearmente independente</strong></li>
</ul>

<p>Tendo em vista o teorema acima, considere a matriz \(A \in \mathbb{R}^n\). Se essa matriz possui \(n\) autovalores linearmente independentes, ou seja, basta aplicar o teorema acima, podemos decompor ela de acordo com os seguintes passos:</p>

<ol>
  <li>Construir uma matriz \(V\) composta pelos \(n\) autovalores, um por coluna: \(V = [ \mathbf{v}_1, \cdots, \mathbf{v}_n]\)</li>
  <li>Da mesma maneira, construir um vetor de autovalores: \(\boldsymbol{\lambda} = [\lambda_1, \cdots, \lambda_n]^T\)</li>
</ol>

<p>A partir dos passos acima, a decomposi√ß√£o √© obtida da seguinte forma:</p>

\[A = V \textrm{diag}(\boldsymbol{\lambda}) V^{-1}
\tag{37}\]

<p>Com j√° foi dito, essa decomposi√ß√£o √© bastante √∫til para analisar algumas propriedades da matriz. Esses conceitos ser√£o melhor abordados quando estivermos estudandando os algoritmos An√°lise de Componentes Principais (PCA) e Decomposi√ß√£o de Valores Singulares (SDV).</p>

<h1 id="considera√ß√µes-finais">Considera√ß√µes finais</h1>
<p>Essa primeira aula √© bastante te√≥rica e muita gente pode achar ela chata. Mas se voc√™ pretende de fato aprender os algoritmos de <em>machine learning</em> para al√©m de uma simples chamada de fun√ß√£o em uma biblioteca, √© importante saber a teoria por tr√°s deles. Portanto, este come√ßo a assim mesmo, mas acredite em mim, ainda vamos ser capazes de fazer bastante coisa interessante neste curso. A p≈ïoxima aula vamos entrar nos principais conceitos de C√°lculo necess√°rios em <em>machine learning</em>.</p>

<h1 id="principais-refer√™ncias">Principais refer√™ncias</h1>
<ul>
  <li>
    <p><em>Deep learning</em> - Ian Goodfellow, Yoshua Bengio e Aaron Courville <a href="https://www.deeplearningbook.org/">link</a></p>
  </li>
  <li>
    <p><em>Linear algebra review and reference</em> - Zico Kolter <a href="http://www.cs.cmu.edu/~zkolter/course/15-884/linalg-review.pdf">link</a></p>
  </li>
  <li>
    <p>Autovetor e Autovalor de um Operador Linear - Luis Crocco <a href="http://www.ufjf.br/luis_crocco/files/2011/09/2014_auto_valores.pdf">link</a></p>
  </li>
</ul>
:ET