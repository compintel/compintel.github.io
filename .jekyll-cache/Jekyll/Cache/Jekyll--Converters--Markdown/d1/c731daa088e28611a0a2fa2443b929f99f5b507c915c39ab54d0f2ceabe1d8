I"°8<h1 id="introdu√ß√£o">Introdu√ß√£o</h1>
<p>Na <a href="/artigos/medidas-classificadores-1/">primeira parte deste post</a> discutimos os conceitos b√°sicos da medida de desempenho de classificadores. Nesta parte vamos discutir basicamente o <em>tradeof</em> entre <em>precision</em> e <em>recall</em>. O objetivo principal √© entender como esse <em>tradeof</em> funciona e dissecar a curva ROC e a √°rea abaixo da curva. Se voc√™ chegou at√© aqui sem ler a parte 1, minha sugest√£o √© que clique no link acima e fa√ßa a leitura antes de continuar.</p>

<p>Para melhor entender esse post vamos utilizar um problema similar ao da parte 1, classificar amostras de c√¢ncer. Nesse caso, vamos considerar apenas uma amostra com C√¢ncer ou n√£o. Vamos manter os mesmos n√∫meros do problema anterior. Para mais informa√ß√µes, leia a parte 1 deste post.</p>

<h1 id="o-tradeof-entre-recall-e-precision">O <em>tradeof</em> entre <em>recall</em> e <em>precision</em></h1>
<p>Como j√° descrito no parte 1, nem sempre vamos conseguir um <em>precision</em> e um <em>recall</em> alto. As vezes ou muitas vezes, vamos teremos um <em>tradeof</em>, que nada mais √© do que um troca, um compromisso. Isso significa que ao aumentar o <em>recall</em>, afetamos o <em>precision</em> e vice versa.</p>

<p>Para entender melhor esse <em>tradeof</em> vamos recorrer a Figura 1 (que foi inspirada em <a href="#hands">[1]</a>). Nessa figura os quadrados vermelhos com a letra C representam amostras que o c√¢ncer retornou. J√° os azuis com a letra O, representa as amostras em que o c√¢ncer n√£o retornou. Portanto, neste mini exemplo temos apenas 12 amostras, que √© suficiente para entender o problema. As setas azuis escuro com valores 1, 2 e 3, representam pontos de <em>thresholds</em>. Como j√° discutimos no post sobre <a href="/artigos/classificacao-de-dados/">classifica√ß√£o de dados</a>, um classificador vai decidir sobre a classe de uma amostra atrav√©s de um <em>threshold</em>, que nada mais do que um limite, um ponto de corte. Se o classificador retorna, por exemplo, uma probabilidade, podemos decidir que o <em>threshold</em> seja 50%, ou seja, se uma amostra tem mais de 50% de chance de n√£o ser um c√¢ncer, a gente optar por este label. Enfim, na figura, cada seta vai representar um valor de <em>threshold</em> que vai afetar no resultado. Para cada seta, a direita teremos amostra classificadas como c√¢ncer e a esquerda como n√£o c√¢ncer. Acima de cada seta est√£o os valores de <em>precision</em>, em laranja, e <em>recall</em> em verde. Caso n√£o tenha entendido de primeira, sugiro que leia esse paragrafo novamente e analisando a figura com mais calma.</p>

<figure style="width: 650px; height: 200px" class="align-center">
  <img src="http://0.0.0.0:4000/assets/img/posts/medidas-desempenho/tradeof.png" />
  <figcaption style="text-align: center;">
    Figura 1: Varia√ß√£o do threshold de decis√£o entre as amostras
  </figcaption>
</figure>

<p>Vamos come√ßar analisando a figura da esquerda para direita. Considerando a seta 1, podemos observar que o <em>recall</em> √© perfeito, ou seja, das 4 amostras classificadas como n√£o c√¢ncer, todas est√£o certas. Portanto, \(R = 100%\). Por outro lado, as 8 amostras classificadas como c√¢ncer, possui 2 erros. Portanto, \(P = 75%\).</p>

<p>Se movermos para a seta 2, ou seja, aumentarmos o valor do <em>threshold</em>, podemos observar aumentamos \(P\) em 5% e dinuimos \(R\) em 33%. Isso por que passamos duas amostras erradas para a equerda e deixamos apenas uma errada na direita. Por fim, se movermos mais um pouco o <em>threshold</em> at√© a seta 3, conseguimos um <em>precision</em> de 100% em troca de um <em>recall</em> de 50%. Conseguiu entender a ess√™ncia do problema? Cada vez que movemos o <em>threshold</em> a gente ganha de um lado mas de perde de outro. Isso √© um <em>tradeof</em> ou compromisso que devemos assumir com o problema. A pergunta natural a se fazer √©: <strong>qual threshold devemos escolher?</strong> Bom, a melhor resposta √© <strong>n√£o sei</strong>, vai depender do problema (como j√° discutimos na parte 1). De qualquer forma, o ideal √© ter em m√£o todas as amostras de valida√ß√£o classificadas para variar o <em>threshold</em> e escolh√™-lo para o seu problema.</p>

<h2 id="plotando-recall-e-precision-x-threshold">Plotando <em>recall</em> e <em>precision</em> x <em>threshold</em></h2>
<p>A visualiza√ß√£o do <em>recall</em> e do <em>precision</em> de acordo com a Figura 1 √© apenas ilustrativa. Para enxergamos de fato como um valor afeta no outro devemos plotar um gr√°ficos em fun√ß√£o do <em>threshold</em>. A ideia √© pegar todos os valores de todas as amostras de um conjunto de treino ou valida√ß√£o e ir variando o <em>threshold</em>, consequentemente, a classifica√ß√£o para todas elas. O resultado √© gr√°fico da Figura 2.</p>

<figure style="width: 500px; height: 280px" class="align-center">
  <img src="http://0.0.0.0:4000/assets/img/posts/medidas-desempenho/r-p-curve.png" />
  <figcaption style="text-align: center;">
    Figura 2: Precision e recall x threshold. Figura retirada de <a href="#hands">[1]</a>
  </figcaption>
</figure>

<p>Com essa curva podemos observar que com <em>threshold</em> igual a zero, temos o maior valor de \(R\) e \(P\), em torno de 80%. A partir da√≠, se aumentarmos o <em>threshold</em> \(P\) aumenta, e \(R\) diminui. Por outro lado, ao diminuirmos, o contr√°rio ocorre.</p>

<h2 id="plotando-precision-x-recall">Plotando <em>Precision</em> x <em>Recall</em></h2>
<p>Uma outra forma de visualizar o <em>tradeof</em> entre as duas m√©tricas √© fazendo um plot de uma versus a outra, como mostrado na Figura 3.</p>

<figure style="width: 500px; height: 380px" class="align-center">
  <img src="http://0.0.0.0:4000/assets/img/posts/medidas-desempenho/r-x-p.png" />
  <figcaption style="text-align: center;">
    Figura 3: Precision x recall. Figura retirada de <a href="#hands">[1]</a>
  </figcaption>
</figure>

<p>Podemos observar que para \(R \approx 0.8\), \(P\) come√ßa a cair drasticamente. Refor√ßando, isso √© uma escolha de projeto. Todavia, voc√™ pode escolher um valor de \(P\) de 90% √© achar que esta tudo bem, certo? A m√° not√≠cia √© que um classificador com \(P\) muito alto n√£o √© t√£o √∫til se \(R\) for muito baixo.</p>

<h1 id="a-curva-roc">A curva ROC</h1>
<p>ROC √© uma abrevia√ß√£o para <em>Receiver Operating Characteristic</em>, no fim das contas √© apenas mais uma maneira de avaliar a qualidade da classifica√ß√£o de dados. Ela √© muito similar a curva <em>precision</em> x <em>recall</em>, por√©m ela √© baseada na curva da <strong>taxa de verdadeiro positivo</strong> (TVP) (outro nome para <em>recall</em>) versus a <strong>taxa de falso positivo</strong> (TFP). Como o pr√≥prio nome sugere, a TFP √© a taxa de inst√¢ncias negativas que s√£o, incorretamente, classificadas como positivas. Ela √© igual a 1 menos a <strong>taxa de verdadeiro negativo</strong>(TVN), que √© a taxa de inst√¢ncias negativas que s√£o, corretamente, classificadas como positivo. Enquanto o <em>recall</em> ou TVP tamb√©m √© conhecida como <em>sensitividade</em>, a TVN √© tamb√©m conhecida como <em>especificidade</em>. Concluindo, a curva ROC √© representada pelo seguinte plot: <code class="language-plaintext highlighter-rouge">Sensitividade (recall)  X  1-Especificidade</code> ou <code class="language-plaintext highlighter-rouge">TVP X TFP</code>.</p>

<p>Eu sei que √© muita sigla. As vezes eu tamb√©m me confudo e tenho que recorrer as minhas anota√ß√µes para lembra. Mas, resumindo tudo:</p>

<ul>
  <li><strong>Taxa de verdadeiro positivo (TVP)</strong>: nada mais √© do que o pr√≥prio <em>recall</em>. √â a taxa de amostras positivas que s√£o corretamente classificadas. Tamb√©m √© conhecida como <strong>sensitividade</strong>*. Definida por \(TVP = \frac{VP}{VP+FN}\)</li>
  <li><strong>Taxa de verdadeiro negativo (TVN)</strong>: √â a taxa de amostras negativas que s√£o corretamente classificadas. Tamb√©m conhecida como <strong>especificidade</strong>. Definida por \(TVN = \frac{VN}{VN+FP}\)</li>
  <li><strong>Taxa de falso positivo (TFP)</strong>: √â a taxa de amostras positivas que s√£o err√¥neamente classificadas. Definida por \(TFP = 1 - TVN = \frac{FP}{VN + FP}\)</li>
</ul>

<h2 id="plotando-a-curva-roc">Plotando a curva ROC</h2>
<p>Para plotar a curva ROC, primeiro √© necess√°rio calcular a TVP e a TFP para diferentes valores do <em>threshold</em>. A curva √© sempre plotada com a TVP no eixo y e a TFP no eixo x. A Figura 4 exibe um exemplo de uma curva ROC.</p>

<figure style="width: 500px; height: 380px" class="align-center">
  <img src="http://0.0.0.0:4000/assets/img/posts/medidas-desempenho/roc.png" />
  <figcaption style="text-align: center;">
    Figura 4: A curva ROC. Figura retirada de <a href="#hands">[1]</a>
  </figcaption>
</figure>

<p>Mais uma vez, essa curva √© um <em>tradeof</em>: quanto maior o <em>recall</em>, ou seja, TVP, mais falso positivos (TFP) o classificador produzir√°. Em outras palavras, subir sensitividade significa baixar a especificidade e vice versa.</p>

<p>A linha tracejada no gr√°fico representa uma curva ROC de um classificador rand√¥mico. Um bom classificador ficar√° o mais longe poss√≠vel desta linha e sempre mirando o topo esquerdo do gr√°fico.</p>

<h2 id="√°rea-abaixo-da-curva-auc">√Årea abaixo da curva (AUC)</h2>
<p>Uma maneira de comparar classificadores √© medindo a √°rea abaixo da curva (AUC, <em>area under the curve</em>). Um classificador perfeito ter√° AUC = 1, pois a ideia √© se aproximar ao m√°ximo do canto superior esquerdo. Quando isso ocorre, um quadrado √© formado e por isso a √°rea ser√° 1 (vide Figura 5). Por outro lado, um classificador aleat√≥rio ter√° AUC = 0.5, como podemos observar na Figura 7. De maneira geral, quanto maior AUC, melhor o classificador.</p>

<h2 id="entendendo-melhor-a-roc">Entendendo melhor a ROC</h2>
<p>Ainda ficou com d√∫vida de como funciona? Ok, vamos a uma an√°lise um pouco mais cuidadosa. Para isso, vamos recorrer a algumas figuras inspiradas em <a href="#unders">[2]</a>. A ROC vai tra√ßar uma curva de acordo com a distribui√ß√£o das amostras do conjunto de dados. Vamos supor que sabemos as distribui√ß√µes da classe positiva (amostras com c√¢ncer) (VP) e das negativas (amostras sem c√¢ncer) (VN). Um classificador com solu√ß√£o ideal √© mostrado na Figura 5.</p>

<figure style="width: 500px; height: 200px" class="align-center">
  <img src="http://0.0.0.0:4000/assets/img/posts/medidas-desempenho/roc-ideal.png" />
  <figcaption style="text-align: center;">
    Figura 5: Exemplo de uma curva ROC ideal
  </figcaption>
</figure>

<p>Observe que os dois conjuntos de amostras s√£o completamente separ√°veis. AUC representa sempre o grau de separabilidade das classificador para com as amostras. Neste caso, AUC = 1, ou seja, completamente separ√°vel. Agora imagine a situa√ß√£o apresentada na Figura 6:</p>

<figure style="width: 500px; height: 200px" class="align-center">
  <img src="http://0.0.0.0:4000/assets/img/posts/medidas-desempenho/roc-07.png" />
  <figcaption style="text-align: center;">
    Figura 6: Exemplo de uma curva ROC com AUC = 0.7
  </figcaption>
</figure>

<p>Neste caso observe que as distribui√ß√µes tem sobreposi√ß√µes, o que faz com que surja amostras falsas positivas a negativas. Neste caso, o classificador n√£o √© mais ideal e AUC = 0.7, o que significa que o modelo tem 70% de chance de distinguir entre as amostras. Obviamente, movendo o <em>threshold</em> vamos priorizar uma das taxas, assim como no <em>tradeof</em> entre <em>recall</em> e <em>precision</em>.</p>

<p>O pr√≥ximo caso que fict√≠cio √© a sobreposi√ß√£o de ambas as distribui√ß√µes, como mostrado na Figura 7.</p>

<figure style="width: 500px; height: 200px" class="align-center">
  <img src="http://0.0.0.0:4000/assets/img/posts/medidas-desempenho/roc-05.png" />
  <figcaption style="text-align: center;">
    Figura 7: Exemplo de uma curva ROC com AUC = 0.5
  </figcaption>
</figure>

<p>Essa √© a pior situa√ß√£o poss√≠vel. Com AUC = 0.5, nosso classificador n√£o sabe distinguir ningu√©m! Ele n√£o possui capacidade de discrimina√ß√£o. Esse √© caso que definitivamente voc√™ n√£o deseja. Por fim, na Figura 8 √© apresentado o caso na qual AUC = 0.</p>

<figure style="width: 500px; height: 200px" class="align-center">
  <img src="http://0.0.0.0:4000/assets/img/posts/medidas-desempenho/roc-0.png" />
  <figcaption style="text-align: center;">
    Figura 8: Exemplo de uma curva ROC com AUC = 0
  </figcaption>
</figure>

<p>Neste caso, em um portugu√™s coloquial, o classificador esta <em>trocando as bolas</em>. Ele classifica quem tem c√¢ncer como que n√£o tem e vice versa. Perceba que esse n√£o √© um caso ruim, muito pelo contr√°rio, pois apenas invertendo a sa√≠da voc√™ possui um classificador perfeito.</p>

<h2 id="como-plotar-roc-e-calcular-auc-para-multiplas-classes">Como plotar ROC e calcular AUC para multiplas classes?</h2>
<p>Assim como no parte 1, para plotar a ROC e calcular a AUC, precisamos isolar as classes e utilizar a metodologia um versus todos. Tomando como exemplo o problema com os labels A, B e C apresentado na parte 1 deste post, o princ√≠pio e basicamente o mesmo do apresentado l√°. Se desejamos plotar a ROC para A, devemos considerar que B e C formam a mesma classe, no caso BC. Por isso que √© um versus todos. √â como se reduzissimos o problema de 3 classes para duas. Perceba que a gente fez isso na parte 1, mas n√£o de maneira t√£o expl√≠cita.</p>

<h1 id="considera√ß√µes-finais">Considera√ß√µes finais</h1>
<p>Nesses dois posts discutimos as medidas de avalia√ß√£o de classificadores. Espero que ap√≥s este longo post voc√™ tenha tirado proveito desses conceitos important√≠ssimos. Alias, caso queira simular/brincar com o problema, <a href="http://www.navan.name/roc/">acesse este simulador de ROC</a>. Como ele voc√™ consegue simular quase todas as situa√ß√µes explanadas nas figuras deste post. Voc√™ consegue mover o <em>threshold</em> e as distribui√ß√µes e verificar como fica a curva ROC.</p>

<p>Ent√£o √© isso! At√© o pr√≥ximo post!</p>

<h1 id="refer√™ncias">Refer√™ncias</h1>
<p><a name="hands">[1]</a> G√©ron, Aur√©lien. Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems. ‚Äú O‚ÄôReilly Media, Inc.‚Äù, 2017.</p>

<p><a name="unders">[2]</a> Understanding AUC - ROC Curve - <a href="https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5">Link</a></p>

:ET